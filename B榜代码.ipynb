{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45a3f6-db11-4ccd-a266-14bd2e99d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import fbeta_score\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65751930-0faf-4512-9f27-9c6275fee521",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0448d4c",
   "metadata": {},
   "source": [
    "读取数据：基础数据表有cust_wid,label,age,gdr_cd,cty_cd几个字段\n",
    "\n",
    "cust_wid是客户id的意思,label:是否会购买理财产品,如果购买是在哪一天购买；可取值0,1,2,...,14;0代表不购买,1,...,14分别代表在1-14天购买\n",
    "\n",
    "age代表年龄;gdr_cd代表性别;cty_cd代表地区或邮政编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fd67f-e790-4633-a7ec-85edbffb23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据：基础数据表有cust_wid,label,age,gdr_cd,cty_cd几个字段\n",
    "#cust_wid是客户id的意思,label:是否会购买理财产品,如果购买是在哪一天购买；可取值0,1,2,...,14;0代表不购买,1,...,14分别代表在1-14天购买\n",
    "#age代表年龄;gdr_cd代表性别;cty_cd代表地区或邮政编码\n",
    "df_train_base = pd.read_csv('/data/train_base.csv')\n",
    "df_test_base = pd.read_csv('/data/testb_base.csv')\n",
    "df_base = pd.concat((df_train_base,df_test_base)) \n",
    "del df_train_base,df_test_base; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79f653-381c-47f3-be0b-a8d28c93bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#我这里犯了一个错误，由于没理解的字段cty_cd的含义,把邮政编码当成了工资这类的东西,所以进行了一个数值型和离散型特征的分离变换\n",
    "df_base['cty_cd_num'] = pd.to_numeric(df_base['cty_cd'], errors='coerce') \n",
    "df_base['cty_cd_cat'] = df_base['cty_cd'].apply(lambda x: x if x in ['A', 'B', 'C'] else 'D' if pd.notna(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2b617-5dfe-4f83-83f3-4201de523ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#缺失值统计\n",
    "for col in df_base.columns:\n",
    "    #总的缺失值\n",
    "    print(f'全体数据{col}的缺失比例',df_base[col].isnull().sum()/df_base.shape[0])\n",
    "    print(f'训练数据{col}的缺失比例',df_base[~df_base['label'].isnull()][col].isnull().sum()/100000)\n",
    "    print(f'测试数据{col}的缺失比例',df_base[df_base['label'].isnull()][col].isnull().sum()/50000)\n",
    "    print(f'负样本{col}的缺失比例',df_base[df_base['label']==0][col].isnull().sum()/df_base[df_base['label']==0].shape[0])\n",
    "    print(f'正样本{col}的缺失比例',df_base[df_base['label']>0][col].isnull().sum()/df_base[df_base['label']>0].shape[0])\n",
    "    print('*'*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6334c-c370-42f7-b025-57e8b0ad996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label比例\n",
    "print('正样本数',df_base[df_base['label']>0].shape[0])\n",
    "print('负样本数',df_base[df_base['label']==0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f8a40-f4ca-437e-8052-2b5a0921cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdr_cd\n",
    "print('训练集男性人数',df_base[(~df_base['label'].isnull())&(df_base['gdr_cd']=='M')].shape[0])\n",
    "print('训练集女性人数',df_base[(~df_base['label'].isnull())&(df_base['gdr_cd']=='F')].shape[0])\n",
    "print('训练集缺失性别人数',df_base[(~df_base['label'].isnull())&(df_base['gdr_cd'].isnull())].shape[0])\n",
    "print('*'*100)\n",
    "print('测试集男性人数',df_base[(df_base['label'].isnull())&(df_base['gdr_cd']=='M')].shape[0])\n",
    "print('测试集女性人数',df_base[(df_base['label'].isnull())&(df_base['gdr_cd']=='F')].shape[0])\n",
    "print('测试集缺失性别人数',df_base[(df_base['label'].isnull())&(df_base['gdr_cd'].isnull())].shape[0])\n",
    "print('*'*100)\n",
    "print('正样本男女比例',df_base[(df_base['label']>0)&(df_base['gdr_cd']=='M')].shape[0]/df_base[(df_base['label']>0)&(df_base['gdr_cd']=='F')].shape[0])\n",
    "print('负样本男女比例',df_base[(df_base['label']==0)&(df_base['gdr_cd']=='M')].shape[0]/df_base[(df_base['label']==0)&(df_base['gdr_cd']=='F')].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f7eb4-03f8-4f74-8d66-ca9fdc778cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cty_cd\n",
    "print('全样本：cty_cd为ABC的总量',df_base['cty_cd'].isin(['A','B','C']).sum())\n",
    "print('全样本：cty_cd为数值',pd.to_numeric(df_base['cty_cd'], errors='coerce').notnull().sum())\n",
    "\n",
    "\n",
    "print('全样本：cty_cd为ABC的分布')\n",
    "count_data = (df_base[df_base['cty_cd'].isin(['A','B','C'])].groupby(['cty_cd'])['cust_wid'].count())\n",
    "plt.bar(count_data.index, count_data.values)\n",
    "for i, value in enumerate(count_data.values):\n",
    "    plt.text(i, value, str(value), ha='center', va='bottom')\n",
    "plt.show()\n",
    "print('全样本cty_cd数值分布')\n",
    "plt.hist(df_base[df_base['cty_cd_num'].notnull()]['cty_cd_num'])\n",
    "plt.show()\n",
    "plt.boxplot(df_base[df_base['cty_cd_num'].notnull()]['cty_cd_num'])\n",
    "plt.show()\n",
    "print('*'*100)\n",
    "print('正样本：cty_cd为ABC的总量',df_base[df_base['label']>0]['cty_cd'].isin(['A','B','C']).sum())\n",
    "print('正样本：cty_cd为数值',df_base[df_base['label']>0]['cty_cd_num'].notnull().sum())\n",
    "\n",
    "\n",
    "print('正样本：cty_cd为ABC的分布')\n",
    "count_data = (df_base[(df_base['cty_cd'].isin(['A','B','C']))&df_base['label']>0].groupby(['cty_cd'])['cust_wid'].count())\n",
    "plt.bar(count_data.index, count_data.values)\n",
    "for i, value in enumerate(count_data.values):\n",
    "    plt.text(i, value, str(value), ha='center', va='bottom')\n",
    "plt.show()\n",
    "print('正样本cty_cd数值分布')\n",
    "\n",
    "plt.hist(df_base[df_base['label']>0]['cty_cd_num'])\n",
    "\n",
    "plt.show()\n",
    "plt.boxplot(df_base[(df_base['cty_cd_num'].notnull())&(df_base['label']>0)]['cty_cd_num'])\n",
    "plt.show()\n",
    "print('*'*100)\n",
    "print('负样本：cty_cd为ABC的总量',df_base[df_base['label']==0]['cty_cd'].isin(['A','B','C']).sum())\n",
    "print('负样本：cty_cd为数值',df_base[df_base['label']==0]['cty_cd_num'].notnull().sum())\n",
    "\n",
    "\n",
    "print('负样本：cty_cd为ABC的分布')\n",
    "count_data = (df_base[(df_base['cty_cd'].isin(['A','B','C']))&(df_base['label']==0)].groupby(['cty_cd'])['cust_wid'].count())\n",
    "plt.bar(count_data.index, count_data.values)\n",
    "for i, value in enumerate(count_data.values):\n",
    "    plt.text(i, value, str(value), ha='center', va='bottom')\n",
    "plt.show()\n",
    "print('负样本cty_cd数值分布')\n",
    "plt.hist(df_base[df_base['label']==0]['cty_cd_num'])\n",
    "\n",
    "plt.show()\n",
    "plt.boxplot(df_base[(df_base['cty_cd_num'].notnull())&(df_base['label']==0)]['cty_cd_num'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2ac4b-22f7-4ea5-a289-5eff27822bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#主表切分训练和测试数据\n",
    "df_train_base = df_base[~df_base['label'].isnull()]\n",
    "df_test_base = df_base[df_base['label'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768eed9-fa61-4fad-9529-521e24642677",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_base;gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3e09194",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a3809c-2bb9-425c-a582-e9823a893c08",
   "metadata": {},
   "source": [
    "## APP 浏览数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "819e221b",
   "metadata": {},
   "source": [
    "APP浏览数据：字段有cust_wid,page_id,acs_tm这几个字段;\n",
    "\n",
    "cust_wid是用户id,\n",
    "\n",
    "page_id是浏览的页面id,\n",
    "\n",
    "acs_tm是用户浏览的时间脱敏结果：脱敏结果精确到秒（实际的时间是啥不清楚）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9eab69-eaa5-48d7-bdc0-555e0551dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_view = pd.read_csv('/data/train_view.csv',encoding = 'ISO-8859-1')\n",
    "df_test_view = pd.read_csv('/data/testb_view.csv',encoding = 'ISO-8859-1')\n",
    "df_view = pd.concat((df_train_view,df_test_view)).reset_index(drop=True)\n",
    "#del df_train_view,df_test_view ;gc.collect()\n",
    "#经过发现交易年份都是在1492年，所以年这个特征没有用，取月日\n",
    "#df_view['acs_tm'] = df_view['acs_tm'].str[5:]\n",
    "df_train_view['acs_date'] = pd.to_datetime(df_train_view['acs_tm'].str[5:10],format='%m-%d')\n",
    "df_test_view['acs_date'] = pd.to_datetime(df_test_view['acs_tm'].str[5:10],format='%m-%d')\n",
    "\n",
    "#(df_view['acs_date'][0]-df_view['acs_date'][2]).days\n",
    "#上下午晚上深夜这种时间区间划分先不搞\n",
    "#日期\n",
    "df_train_view['acs_d'] = pd.to_numeric(df_train_view['acs_tm'].str[8:10], errors='coerce')\n",
    "df_test_view['acs_d'] = pd.to_numeric(df_test_view['acs_tm'].str[8:10], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc546c38-0821-49e3-a334-6c7b641cbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取浏览量最多的前100哥page_id\n",
    "hot_pid = df_view.groupby('page_id')['cust_wid'].count().sort_values( ascending=False).head(100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd80eff-9062-452b-b355-cb5d7547d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('page_id共有',len(df_view['page_id'].unique()))\n",
    "#print('cust_wid共有',len(df_view['cust_wid'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12f0c2-9009-4810-9dbf-7c8ce2e49d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_view['acs_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f9cdf-69e6-488e-99fb-89cf6c5c1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.concat([df_train_base,df_test_base],axis= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2939bb-37f1-4d43-a526-c26aab31c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#关联base表的label到drain_view表上,方便计算page_id的编码\n",
    "df_train_view = df_train_view.merge(df_base[['cust_wid','label']],how='left',on='cust_wid')\n",
    "df_train_view['label'] = df_train_view['label'].apply(lambda x:0 if x==0 else 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c47213-6043-49ca-885d-f9bd5fab97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_view"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1525ba0",
   "metadata": {},
   "source": [
    "接下来,要做本工作中最重要的一部分特征：对page_id的特征处理。page_id特征的构造很大程度的影响了模型的效果。\n",
    "\n",
    "从业务直观上来讲,用户浏览的页面信息对用户是否在将来会购买理财产品的影响应该不大,但是在本题中影响很大。\n",
    "\n",
    "下面介绍一种从数据驱动的方式找出这一重要特征的方法：类别特征编码。类别特征编码的方法有很多种，从编码的结果可以看出类别特征的每个取值在正负样本中的分布有无比较大的区别。我采用了target encoding(一种后验概率),发现：page_id的target_encoding分布在0附近和1附近的非常多，说明许多page_id是能够区分正负样本的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84432f0-2c36-480f-9aee-59aca88a04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要进行target encoding的列名\n",
    "cat_cols = [\"page_id\"]\n",
    "\n",
    "# 创建target encoder对象，并进行target encoding\n",
    "encoder = ce.TargetEncoder(cols=cat_cols)\n",
    "encoder.fit(df_train_view[cat_cols], df_train_view[\"label\"])\n",
    "data_encoded = encoder.transform(df_train_view[cat_cols]).add_suffix(\"_target\")\n",
    "df_train_view = pd.concat([df_train_view, data_encoded], axis=1)\n",
    "#取target encoding 小于0.3和target encoding大于0.7的page_id，当作重要的page_id\n",
    "important_page_id = pd.unique(df_train_view[(df_train_view['page_id_target']<0.3)| (df_train_view['page_id_target']>0.7) ]['page_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8eb5eb-644a-4a4e-9ca6-2cd47071b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a06417-15f9-42a2-899a-f66a4593bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把hot100的page_id和target_encoding后的重要id合并\n",
    "important_page_id = list(set(important_page_id)|set(hot_pid))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e613ad-ce81-4c6e-8a1a-3cad6dca1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(important_page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3956695-cc5d-49fa-b0d0-39aa7234b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造一个中间表，该表记录每个page_id和对应的page_id的target_encoding\n",
    "df_page_targetencode = df_train_view.groupby('page_id')['page_id_target'].max().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f81e16-8007-4448-af0d-fdecb360bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page_targetencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4d3df-84ff-46ed-be9c-325a25e4239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把df_test_view关联上page_id的target_encoding\n",
    "df_test_view = df_test_view.merge(df_page_targetencode,how='left',on='page_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e8b83-919d-4aca-a6ca-9147520eedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa07edb-025c-45b0-bac4-456417aaee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view = pd.concat((df_train_view,df_test_view)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543dcc0f-b1e9-4a13-9fd7-421d0c7e89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计每个page_id被浏览的数量，并关联到view表上，以便之后做衍生特征\n",
    "temp = df_view.groupby('page_id')['cust_wid'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'cust_wid':'page_nums'},inplace=True)\n",
    "\n",
    "df_view = df_view.merge(temp,how='left',on='page_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad8a35d-36ed-434a-86c4-1ece1bbdd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37da06b-d95b-49fc-9dc6-9906e979a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_features = pd.DataFrame(df_base['cust_wid'])\n",
    "#用户共访问了多少次\n",
    "temp_view_count = pd.DataFrame(df_view.groupby('cust_wid')['page_id'].count()).reset_index(drop=False)\n",
    "temp_view_count.rename(columns = {'page_id':'view_count'},inplace=True)\n",
    "view_features = view_features.merge(temp_view_count,how='left',on='cust_wid')\n",
    "del temp_view_count;gc.collect()\n",
    "#用户访问了多少个Page_id\n",
    "temp_pageid_count = pd.DataFrame(df_view.groupby('cust_wid')['page_id'].nunique()).reset_index(drop=False)\n",
    "temp_pageid_count.rename(columns = {'page_id':'page_count'},inplace=True)\n",
    "view_features= view_features.merge(temp_pageid_count,how='left',on='cust_wid')\n",
    "del temp_pageid_count;gc.collect()\n",
    "#8月(脱敏)有多少天访问了\n",
    "temp_days_count = pd.DataFrame(df_view.groupby('cust_wid')['acs_d'].nunique()).reset_index(drop=False)\n",
    "temp_days_count.rename(columns = {'acs_d':'acs_days_count'},inplace=True)\n",
    "view_features= view_features.merge(temp_days_count,how='left',on='cust_wid')\n",
    "del temp_days_count;gc.collect()\n",
    "#用户在一天(脱敏)内最多访问多少次\n",
    "temp_oneday_views = df_view.groupby(['cust_wid','acs_d'])['page_id'].count().groupby(['cust_wid']).max().reset_index(drop=False)\n",
    "temp_oneday_views.rename(columns = {'page_id':'max_oneday_views'},inplace=True)\n",
    "view_features= view_features.merge(temp_oneday_views,how='left',on='cust_wid')\n",
    "del temp_oneday_views;gc.collect()\n",
    "#用户在一天内(脱敏)最少访问多少次\n",
    "temp_oneday_views = df_view.groupby(['cust_wid','acs_d'])['page_id'].count().groupby(['cust_wid']).min().reset_index(drop=False)\n",
    "temp_oneday_views.rename(columns = {'page_id':'min_oneday_views'},inplace=True)\n",
    "view_features= view_features.merge(temp_oneday_views,how='left',on='cust_wid')\n",
    "del temp_oneday_views;gc.collect()\n",
    "#用户在一天最多访问多少个page_id\n",
    "temp_oneday_pages = df_view.groupby(['cust_wid','acs_d'])['page_id'].nunique().groupby(['cust_wid']).max().reset_index(drop=False)\n",
    "temp_oneday_pages.rename(columns = {'page_id':'max_oneday_pages'},inplace=True)\n",
    "view_features= view_features.merge(temp_oneday_pages,how='left',on='cust_wid')\n",
    "del temp_oneday_pages;gc.collect()\n",
    "#用户在一天最少访问多少个page_id\n",
    "temp_oneday_pages = df_view.groupby(['cust_wid','acs_d'])['page_id'].nunique().groupby(['cust_wid']).min().reset_index(drop=False)\n",
    "temp_oneday_pages.rename(columns = {'page_id':'min_oneday_pages'},inplace=True)\n",
    "view_features= view_features.merge(temp_oneday_pages,how='left',on='cust_wid')\n",
    "del temp_oneday_pages;gc.collect()\n",
    "\n",
    "#8月上旬(脱敏)访问了多少次\n",
    "temp_shangxun_count = df_view[df_view['acs_d']<=10].groupby('cust_wid')['page_id'].count().reset_index(drop=False)\n",
    "temp_shangxun_count.rename(columns = {'page_id':'shangxun_view_count'},inplace=True)\n",
    "view_features= view_features.merge(temp_shangxun_count,how='left',on='cust_wid')\n",
    "del temp_shangxun_count;gc.collect()\n",
    "#8月中旬(脱敏)访问了多少次\n",
    "temp_zhongxun_count = df_view[(df_view['acs_d']<=20)&(df_view['acs_d']>10)].groupby('cust_wid')['page_id'].count().reset_index(drop=False)\n",
    "temp_zhongxun_count.rename(columns = {'page_id':'zhongxun_view_count'},inplace=True)\n",
    "view_features= view_features.merge(temp_zhongxun_count,how='left',on='cust_wid')\n",
    "del temp_zhongxun_count;gc.collect()\n",
    "#8月下旬(脱敏)访问了多少次\n",
    "temp_xiaxun_count = df_view[df_view['acs_d']>20].groupby('cust_wid')['page_id'].count().reset_index(drop=False)\n",
    "temp_xiaxun_count.rename(columns = {'page_id':'xiaxun_view_count'},inplace=True)\n",
    "view_features= view_features.merge(temp_xiaxun_count,how='left',on='cust_wid')\n",
    "del temp_xiaxun_count;gc.collect()\n",
    "#8月上旬(脱敏)访问了多少个page\n",
    "temp_shangxun_page = df_view[df_view['acs_d']<=10].groupby('cust_wid')['page_id'].nunique().reset_index(drop=False)\n",
    "temp_shangxun_page.rename(columns = {'page_id':'shangxun_page'},inplace=True)\n",
    "view_features= view_features.merge(temp_shangxun_page,how='left',on='cust_wid')\n",
    "del temp_shangxun_page;gc.collect()\n",
    "#8月中旬(脱敏)访问了多少pages\n",
    "temp_zhongxun_page = df_view[(df_view['acs_d']<=20)&(df_view['acs_d']>10)].groupby('cust_wid')['page_id'].nunique().reset_index(drop=False)\n",
    "temp_zhongxun_page.rename(columns = {'page_id':'zhongxun_page'},inplace=True)\n",
    "view_features= view_features.merge(temp_zhongxun_page,how='left',on='cust_wid')\n",
    "del temp_zhongxun_page;gc.collect()\n",
    "#8月下旬(脱敏)访问了多少个page\n",
    "temp_xiaxun_page = df_view[df_view['acs_d']>20].groupby('cust_wid')['page_id'].nunique().reset_index(drop=False)\n",
    "temp_xiaxun_page.rename(columns = {'page_id':'xiaxun_page'},inplace=True)\n",
    "view_features= view_features.merge(temp_xiaxun_page,how='left',on='cust_wid')\n",
    "del temp_xiaxun_page;gc.collect()\n",
    "#对于重要page_id，分别统计用户浏览了多少次\n",
    "temp = df_view[df_view['page_id'].isin(important_page_id)].groupby('cust_wid')['page_id'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'page_id':'important_page_cnt'},inplace=True)\n",
    "view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "del temp;gc.collect()\n",
    "for pid in important_page_id:\n",
    "    temp = df_view[df_view['page_id']==pid].groupby('cust_wid')['page_id'].count().reset_index(drop=False)\n",
    "    temp.rename(columns = {'page_id':f'page_{pid}_cnt'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#这个填空值是我随便写的，可能不重要吧\n",
    "view_features[['shangxun_view_count','zhongxun_view_count','xiaxun_view_count',\n",
    "                    'max_oneday_views','max_oneday_pages',\n",
    "                    'shangxun_page','zhongxun_page','xiaxun_page']] =view_features[['shangxun_view_count','zhongxun_view_count','xiaxun_view_count',\n",
    "                                                                                 'max_oneday_views','max_oneday_pages',\n",
    "                                                                                'shangxun_page','zhongxun_page','xiaxun_page']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e53dc94-801f-485a-a356-f449f57a13b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8f988-d144-4c51-af52-ff50591e4e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f59d7-940e-47d3-bf1f-19ee0ab66833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_view_feature.to_csv('/work/train_view_0.csv')\n",
    "#test_view_feature.to_csv('/work/test_view_0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04e3fb-cfd2-4c5e-91e9-baf57ea90260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_view_feature = pd.read_csv('train_view_0.csv',index_col=0)\n",
    "#test_view_feature = pd.read_csv('test_view_0.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad16d87-7f7a-4a64-bc34-98dc253ec659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#用户共访问了多少次重要Page\n",
    "temp = df_view[df_view['page_id'].isin(important_page_id)].groupby('cust_wid')['acs_d'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'acs_d':'important_page_cnt'},inplace=True)\n",
    "view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "del temp;gc.collect()\n",
    "#用户在一个月内访问了重要page的共有几天\n",
    "temp = df_view[df_view['page_id'].isin(important_page_id)].groupby('cust_wid')['acs_d'].nunique().reset_index(drop=False)\n",
    "temp.rename(columns = {'acs_d':'important_page_ndays'},inplace=True)\n",
    "view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705ac2c-d91a-493f-99e3-2a715c4c6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对所有用户访问page_id做target_encoding统计量特征\n",
    "index = ['mean','sum','max','min','std']\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['page_id_target'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['page_id_target'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view.groupby('cust_wid')['page_id_target'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view.groupby('cust_wid')['page_id_target'].min().reset_index(drop=False)\n",
    "    elif ind =='sum':\n",
    "        temp = df_view.groupby('cust_wid')['page_id_target'].sum().reset_index(drop=False)\n",
    "    temp.rename(columns = {'page_id_target':f'page_id_target_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fea779-1875-45f4-b813-12731bc3b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8月上旬对所有用户访问page_id做target_encoding统计量特征\n",
    "index = ['mean','sum','max','min','std']\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view[df_view['acs_d']<=10].groupby('cust_wid')['page_id_target'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view[df_view['acs_d']<=10].groupby('cust_wid')['page_id_target'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view[df_view['acs_d']<=10].groupby('cust_wid')['page_id_target'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view[df_view['acs_d']<=10].groupby('cust_wid')['page_id_target'].min().reset_index(drop=False)\n",
    "    elif ind =='sum':\n",
    "        temp = df_view[df_view['acs_d']<=10].groupby('cust_wid')['page_id_target'].sum().reset_index(drop=False)\n",
    "    temp.rename(columns = {'page_id_target':f'shangxun_page_id_target_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()    \n",
    "\n",
    "    \n",
    "#8月中旬对所有用户访问page_id做target_encoding统计量特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view[(df_view['acs_d']<=20)&(df_view['acs_d']>10)].groupby('cust_wid')['page_id_target'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view[(df_view['acs_d']<=20)&(df_view['acs_d']>10)].groupby('cust_wid')['page_id_target'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view[(df_view['acs_d']<=20)&(df_view['acs_d']>10)].groupby('cust_wid')['page_id_target'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view[(df_view['acs_d']<=20)&(df_view['acs_d']>10)].groupby('cust_wid')['page_id_target'].min().reset_index(drop=False)\n",
    "    elif ind =='sum':\n",
    "        temp = df_view[(df_view['acs_d']<=20)&(df_view['acs_d']>10)].groupby('cust_wid')['page_id_target'].sum().reset_index(drop=False)\n",
    "    temp.rename(columns = {'page_id_target':f'zhongxun_page_id_target_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()    \n",
    "\n",
    "#8月下旬对所有用户访问page_id做target_encoding统计量特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view[df_view['acs_d']>20].groupby('cust_wid')['page_id_target'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp =  df_view[df_view['acs_d']>20].groupby('cust_wid')['page_id_target'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp =  df_view[df_view['acs_d']>20].groupby('cust_wid')['page_id_target'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp =  df_view[df_view['acs_d']>20].groupby('cust_wid')['page_id_target'].min().reset_index(drop=False)\n",
    "    elif ind =='sum':\n",
    "        temp =  df_view[df_view['acs_d']>20].groupby('cust_wid')['page_id_target'].sum().reset_index(drop=False)\n",
    "    temp.rename(columns = {'page_id_target':f'xiaxun_page_id_target_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ecc6d4-a81b-471a-a065-1569612550c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d2ca6-7f9b-4ba3-bc45-bdfd6e8f9614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551fc1e-3445-4bf1-a745-455194e62654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c31a6-a35b-4d8f-803c-b8070e134096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628452a-d271-4e95-9bc0-aac3c32126b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda063b-ca74-45d3-a712-e9657a343d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29d759-1083-454e-a211-af0a200696ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对每个Page_id的访问量进行计数并关联到view表,方便做衍生特征\n",
    "temp_grouppage_count = df_view.groupby('page_id')['acs_d'].count().reset_index(drop=False)\n",
    "temp_grouppage_count.rename(columns = {'acs_d':'group_page_cnt'},inplace=True)\n",
    "df_view = df_view.merge(temp_grouppage_count,how='left',on='page_id')\n",
    "del temp_grouppage_count;gc.collect()\n",
    "#对page_id访问量做统计特征\n",
    "index = ['mean','std','max','min']\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'group_page_cnt':f'group_page_cnt_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "    \n",
    "    \n",
    "#计算每个Page_id被多少人访问,并关联到view表上\n",
    "temp_grouppage_uid_cnt = df_view.groupby('page_id')['cust_wid'].nunique().reset_index(drop=False) \n",
    "temp_grouppage_uid_cnt.rename(columns = {'cust_wid':'group_page_uid_cnt'},inplace=True)\n",
    "df_view = df_view.merge(temp_grouppage_uid_cnt,how='left',on='page_id')\n",
    "del temp_grouppage_uid_cnt;gc.collect()\n",
    "#对page_id访问人数做统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_uid_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_uid_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_uid_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_uid_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'group_page_uid_cnt':f'group_page_uid_cnt_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "#对每个page_id计算被访问的天数,并关联到view表上\n",
    "temp_grouppage_days_count = df_view.groupby('page_id')['acs_d'].nunique().reset_index(drop=False)\n",
    "temp_grouppage_days_count.rename(columns = {'acs_d':'group_page_days_cnt'},inplace=True)\n",
    "df_view = df_view.merge(temp_grouppage_days_count,how='left',on='page_id')\n",
    "del temp_grouppage_days_count;gc.collect()\n",
    "#对page_id被访问的天数做统计量特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_days_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_days_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_days_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view.groupby('cust_wid')['group_page_days_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'group_page_days_cnt':f'group_page_days_cnt_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#对每个用户和每个page_id的组合分组,计算浏览次数，并关联到view表\n",
    "temp = df_view.groupby(['cust_wid','page_id'])['acs_tm'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'acs_tm':'groupby_uid_pid_cnt'},inplace=True)\n",
    "df_view = df_view.merge(temp,how='left',on=['cust_wid','page_id'])\n",
    "del temp;gc.collect()\n",
    "#对每个用户和每个page_id的组合分组浏览次数做统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_pid_cnt':f'groupby_uid_pid_cnt_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "\n",
    "#对每个用户和每个page_id的组合分组,计算浏览天数，并关联到view表\n",
    "temp = df_view.groupby(['cust_wid','page_id'])['acs_d'].nunique().reset_index(drop=False)\n",
    "temp.rename(columns = {'acs_d':'groupby_uid_pid_days_cnt'},inplace=True)\n",
    "df_view = df_view.merge(temp,how='left',on=['cust_wid','page_id'])\n",
    "del temp;gc.collect()\n",
    "#对每个用户和每个page_id的组合分组浏览天数做统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_days_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_days_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_days_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_pid_days_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_pid_days_cnt':f'groupby_uid_pid_days_cnt_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#对每个用户每天组合分组，计算浏览次数，并关联到view表\n",
    "temp = df_view.groupby(['cust_wid','acs_date'])['page_id'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'page_id':'groupby_uid_date_cnt'},inplace=True)\n",
    "df_view = df_view.merge(temp,how='left',on=['cust_wid','acs_date'])\n",
    "del temp;gc.collect()\n",
    "#对每个用户每天组合分组浏览次数做统计特征\n",
    "for ind in index:\n",
    "    if ind =='max':\n",
    "        break \n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_date_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_date_cnt'].std().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_date_cnt':f'groupby_uid_date_cnt_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#计算每个用户每天组合分组的浏览page个数,并关联到view表\n",
    "temp = df_view.groupby(['cust_wid','acs_date'])['page_id'].nunique().reset_index(drop=False)\n",
    "temp.rename(columns = {'page_id':'groupby_uid_date_page_cnt'},inplace=True)\n",
    "df_view = df_view.merge(temp,how='left',on=['cust_wid','acs_date'])\n",
    "del temp;gc.collect()\n",
    "#每个用户每天组合分组的浏览page个数统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_date_page_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_date_page_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_date_page_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_view.groupby('cust_wid')['groupby_uid_date_page_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_date_page_cnt':f'groupby_uid_date_page_cnt_{ind}'},inplace=True)\n",
    "    view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afbb94-1838-49f6-aeae-8042871b6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_features.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680abe74-e649-430e-8ee0-8ccf72e4de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分别统计每天的page_id target_encoding的统计特征\n",
    "dates = df_view['acs_d'].unique()\n",
    "index = ['sum','mean','std','max','min']\n",
    "for d in dates:\n",
    "    for ind in index:\n",
    "        if ind == 'mean':\n",
    "            temp = df_view.groupby('cust_wid')['page_id_target'].mean().reset_index(drop=False)\n",
    "        elif ind=='std':\n",
    "            temp = df_view.groupby('cust_wid')['page_id_target'].std().reset_index(drop=False)\n",
    "        elif ind =='max':\n",
    "            temp = df_view.groupby('cust_wid')['page_id_target'].max().reset_index(drop=False)\n",
    "        elif ind =='min':\n",
    "            temp = df_view.groupby('cust_wid')['page_id_target'].min().reset_index(drop=False)\n",
    "        elif ind =='sum':\n",
    "            temp = df_view.groupby('cust_wid')['page_id_target'].sum().reset_index(drop=False)\n",
    "        temp.rename(columns = {'page_id_target':f'date_{d}_pagetargetencode_{ind}'},inplace=True)\n",
    "        view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "        del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603e027-14e9-477a-a1f9-955e993f6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分别统计每天page_id的访问量的统计特征\n",
    "dates = df_view['acs_d'].unique()\n",
    "index = ['sum','mean','std','max','min']\n",
    "for d in dates:\n",
    "    for ind in index:\n",
    "        if ind == 'mean':\n",
    "            temp = df_view.groupby('cust_wid')['page_nums'].mean().reset_index(drop=False)\n",
    "        elif ind=='std':\n",
    "            temp = df_view.groupby('cust_wid')['page_nums'].std().reset_index(drop=False)\n",
    "        elif ind =='max':\n",
    "            temp = df_view.groupby('cust_wid')['page_nums'].max().reset_index(drop=False)\n",
    "        elif ind =='min':\n",
    "            temp = df_view.groupby('cust_wid')['page_nums'].min().reset_index(drop=False)\n",
    "        elif ind =='sum':\n",
    "            temp = df_view.groupby('cust_wid')['page_nums'].sum().reset_index(drop=False)\n",
    "        temp.rename(columns = {'page_nums':f'date_{d}_page_nums_{ind}'},inplace=True)\n",
    "        view_features = view_features.merge(temp,how='left',on='cust_wid')\n",
    "        del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e4a90-6994-4d43-84c2-eb36d655dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_features.to_parquet('/work/view_features_b.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e6513-c418-4134-b986-31cead1c6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train_view,df_test_view;df_view;view_features;gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "024042e1-35c4-4aab-a8ca-f4b250443e29",
   "metadata": {},
   "source": [
    "## 收支交易数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "def4bcad",
   "metadata": {},
   "source": [
    "交易数据有cust_wid,trx_cd,trx_tm,trx_amt这几个字段，分别代表用户id,交易行为的id,交易金额(脱敏),交易时间\n",
    "\n",
    "该部分特征重要性比较低。虽然从直观上理解，交易的行为应该是影响用户是否会在将来购买理财产品的重要特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8e819-6359-4b07-90a3-f6451057f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_trx = pd.read_csv('/data/train_trx.csv',encoding = 'ISO-8859-1')\n",
    "df_test_trx = pd.read_csv('/data/testb_trx.csv',encoding = 'ISO-8859-1')\n",
    "df_train_trx['trx_date'] = pd.to_datetime(df_train_trx['trx_tm'].str[5:10],format='%m-%d')\n",
    "df_test_trx['trx_date'] = pd.to_datetime(df_test_trx['trx_tm'].str[5:10],format='%m-%d')\n",
    "\n",
    "#(df_view['acs_date'][0]-df_view['acs_date'][2]).days\n",
    "#上下午晚上深夜这种时间区间划分先不搞\n",
    "#日期\n",
    "df_train_trx['trx_d'] = pd.to_numeric(df_train_trx['trx_tm'].str[8:10], errors='coerce')\n",
    "df_test_trx['trx_d'] = pd.to_numeric(df_test_trx['trx_tm'].str[8:10], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4b7f3-8dc0-420f-917b-85d56016fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(inter_cid,columns=['nan_cust_wid']).to_csv('/work/test_data/nan_cid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca94b8-dbae-4f50-aeda-0ef676de6024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trx = pd.concat((df_train_trx,df_test_trx)).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8673a7e2",
   "metadata": {},
   "source": [
    "同样对trx_cd做target encoding,但是发现target_encoding都在0.5附近,证明trx_cd对正负样本没有太大区分度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d77ab0-6769-4c1e-9d58-297a99b8be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "df_train_trx = df_train_trx.merge(df_base[['cust_wid','label']],how='left',on='cust_wid')\n",
    "df_train_trx['label'] = df_train_trx['label'].apply(lambda x:0 if x==0 else 1 )\n",
    "# 定义要进行target encoding的列名\n",
    "cat_cols = [\"trx_cd\"]\n",
    "# 创建target encoder对象，并进行target encoding\n",
    "encoder = ce.TargetEncoder(cols=cat_cols)\n",
    "encoder.fit(df_train_trx[cat_cols], df_train_trx[\"label\"])\n",
    "data_encoded = encoder.transform(df_train_trx[cat_cols]).add_suffix(\"_target\")\n",
    "df_train_trx = pd.concat([df_train_trx, data_encoded], axis=1)\n",
    "important_trx_cd = pd.unique(df_train_trx[df_train_trx['trx_cd_target']>0.7 ]['trx_cd'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e25b24f4-f3ad-4f77-a8ac-799b5bd66f7b",
   "metadata": {},
   "source": [
    "仿造view表构造一系列特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcf240-c936-470b-9e2b-6d21169c15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_features = pd.DataFrame(df_base['cust_wid'])\n",
    "#temp_trx_count访问了多少次\n",
    "temp_trx_count = pd.DataFrame(df_trx.groupby('cust_wid')['trx_cd'].count()).reset_index(drop=False)\n",
    "temp_trx_count.rename(columns = {'trx_cd':'trx_count'},inplace=True)\n",
    "trx_features = trx_features.merge(temp_trx_count,how='left',on='cust_wid')\n",
    "del temp_trx_count;gc.collect()\n",
    "#交易了多少种cd\n",
    "temp_cd_count = pd.DataFrame(df_trx.groupby('cust_wid')['trx_cd'].nunique()).reset_index(drop=False)\n",
    "temp_cd_count.rename(columns = {'trx_cd':'trx_cd_count'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_cd_count,how='left',on='cust_wid')\n",
    "del temp_cd_count;gc.collect()\n",
    "#8月有多少天交易\n",
    "temp_days_count = pd.DataFrame(df_trx.groupby('cust_wid')['trx_d'].nunique()).reset_index(drop=False)\n",
    "temp_days_count.rename(columns = {'trx_d':'trx_days_count'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_days_count,how='left',on='cust_wid')\n",
    "del temp_days_count;gc.collect()\n",
    "#用户在一天内最多访问多少次\n",
    "temp_oneday_trx = df_trx.groupby(['cust_wid','trx_d'])['trx_cd'].count().groupby(['cust_wid']).max().reset_index(drop=False)\n",
    "temp_oneday_trx.rename(columns = {'trx_cd':'max_oneday_trx'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_oneday_trx,how='left',on='cust_wid')\n",
    "del temp_oneday_trx;gc.collect()\n",
    "#用户在一天内最少访问多少次\n",
    "temp_oneday_trx = df_trx.groupby(['cust_wid','trx_d'])['trx_cd'].count().groupby(['cust_wid']).min().reset_index(drop=False)\n",
    "temp_oneday_trx.rename(columns = {'trx_cd':'min_oneday_trx'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_oneday_trx,how='left',on='cust_wid')\n",
    "del temp_oneday_trx;gc.collect()\n",
    "#用户在一天最多访问多少个trx_id\n",
    "temp_oneday_trxid = df_trx.groupby(['cust_wid','trx_d'])['trx_cd'].nunique().groupby(['cust_wid']).max().reset_index(drop=False)\n",
    "temp_oneday_trxid.rename(columns = {'trx_cd':'max_oneday_trxcd'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_oneday_trxid,how='left',on='cust_wid')\n",
    "del temp_oneday_trxid;gc.collect()\n",
    "#用户在一天最少访问多少个trx_id\n",
    "temp_oneday_trxid = df_trx.groupby(['cust_wid','trx_d'])['trx_cd'].nunique().groupby(['cust_wid']).min().reset_index(drop=False)\n",
    "temp_oneday_trxid.rename(columns = {'trx_cd':'min_oneday_trxcd'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_oneday_trxid,how='left',on='cust_wid')\n",
    "del temp_oneday_trxid;gc.collect()\n",
    "#8月上旬交易了多少次\n",
    "temp_shangxun_count = df_trx[df_trx['trx_d']<=10].groupby('cust_wid')['trx_cd'].count().reset_index(drop=False)\n",
    "temp_shangxun_count.rename(columns = {'trx_cd':'shangxun_trx_count'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_shangxun_count,how='left',on='cust_wid')\n",
    "del temp_shangxun_count;gc.collect()\n",
    "#8月中旬访问了多少次\n",
    "temp_zhongxun_count = df_trx[(df_trx['trx_d']<=20)&(df_trx['trx_d']>10)].groupby('cust_wid')['trx_cd'].count().reset_index(drop=False)\n",
    "temp_zhongxun_count.rename(columns = {'trx_cd':'zhongxun_trx_count'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_zhongxun_count,how='left',on='cust_wid')\n",
    "del temp_zhongxun_count;gc.collect()\n",
    "#8月下旬访问了多少次\n",
    "temp_xiaxun_count = df_trx[df_trx['trx_d']>20].groupby('cust_wid')['trx_cd'].count().reset_index(drop=False)\n",
    "temp_xiaxun_count.rename(columns = {'trx_cd':'xiaxun_trx_count'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_xiaxun_count,how='left',on='cust_wid')\n",
    "del temp_xiaxun_count;gc.collect()\n",
    "#交易总量\n",
    "temp_trx_amtall = df_trx.groupby('cust_wid')['trx_amt'].sum().reset_index(drop=False)\n",
    "temp_trx_amtall.rename(columns = {'trx_amt':'amt_all'},inplace=True)\n",
    "trx_features= trx_features.merge(temp_trx_amtall,how='left',on='cust_wid')\n",
    "plt.boxplot(trx_features['amt_all'])\n",
    "plt.show()\n",
    "del temp_trx_amtall;gc.collect()\n",
    "\n",
    "#交易均值\n",
    "temp_trx_amtmean = df_trx.groupby('cust_wid')['trx_amt'].mean().reset_index(drop=False)\n",
    "temp_trx_amtmean.rename(columns = {'trx_amt':'amt_mean'},inplace=True)\n",
    "temp_trx_amtmean.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_trx_amtmean,how='left',on='cust_wid')\n",
    "plt.boxplot(trx_features['amt_mean'])\n",
    "plt.show()\n",
    "del temp_trx_amtmean;gc.collect()\n",
    "#交易方差\n",
    "temp_trx_amtstd = df_trx.groupby('cust_wid')['trx_amt'].std().reset_index(drop=False)\n",
    "temp_trx_amtstd.rename(columns = {'trx_amt':'amt_std'},inplace=True)\n",
    "temp_trx_amtstd.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_trx_amtstd,how='left',on='cust_wid')\n",
    "plt.boxplot(trx_features['amt_std'])\n",
    "plt.show()\n",
    "del temp_trx_amtstd;gc.collect()\n",
    "#交易最大值\n",
    "temp_trx_amtmax = df_trx.groupby('cust_wid')['trx_amt'].max().reset_index(drop=False)\n",
    "temp_trx_amtmax.rename(columns = {'trx_amt':'amt_max'},inplace=True)\n",
    "temp_trx_amtmax.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_trx_amtmax,how='left',on='cust_wid')\n",
    "plt.boxplot(trx_features['amt_max'])\n",
    "plt.show()\n",
    "del temp_trx_amtmax;gc.collect()\n",
    "#交易最小值\n",
    "temp_trx_amtmin = df_trx.groupby('cust_wid')['trx_amt'].min().reset_index(drop=False)\n",
    "temp_trx_amtmin.rename(columns = {'trx_amt':'amt_min'},inplace=True)\n",
    "temp_trx_amtmin.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_trx_amtmin,how='left',on='cust_wid')\n",
    "plt.boxplot(trx_features['amt_min'])\n",
    "plt.show()\n",
    "del temp_trx_amtmin;gc.collect()\n",
    "#8月上旬交易总量\n",
    "temp_shangxun_amtsum = df_trx[df_trx['trx_d']<=10].groupby('cust_wid')['trx_amt'].sum().reset_index(drop=False)\n",
    "temp_shangxun_amtsum.rename(columns = {'trx_amt':'shangxun_trx_amtsum'},inplace=True)\n",
    "#temp_shangxun_amtsum.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_shangxun_amtsum,how='left',on='cust_wid')\n",
    "del temp_shangxun_amtsum;gc.collect()\n",
    "#8月中旬交易总量\n",
    "temp_zhongxun_amtsum = df_trx[(df_trx['trx_d']<=20)&(df_trx['trx_d']>10)].groupby('cust_wid')['trx_amt'].sum().reset_index(drop=False)\n",
    "temp_zhongxun_amtsum.rename(columns = {'trx_amt':'zhongxun_trx_amt_sum'},inplace=True)\n",
    "#temp_zhongxun_amtsum.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_zhongxun_amtsum,how='left',on='cust_wid')\n",
    "del temp_zhongxun_amtsum;gc.collect()\n",
    "#8月下旬交易总量\n",
    "temp_xiaxun_amtsum = df_trx[df_trx['trx_d']>20].groupby('cust_wid')['trx_amt'].sum().reset_index(drop=False)\n",
    "temp_xiaxun_amtsum.rename(columns = {'trx_amt':'xiaxun_trx_amt_sum'},inplace=True)\n",
    "#temp_xiaxun_amtsum.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_xiaxun_amtsum,how='left',on='cust_wid')\n",
    "del temp_xiaxun_amtsum;gc.collect()\n",
    "#8月上旬交易均值\n",
    "temp_shangxun_amtmean = df_trx[df_trx['trx_d']<=10].groupby('cust_wid')['trx_amt'].mean().reset_index(drop=False)\n",
    "temp_shangxun_amtmean.rename(columns = {'trx_amt':'shangxun_trx_amtmean'},inplace=True)\n",
    "#temp_shangxun_amtsum.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_shangxun_amtmean,how='left',on='cust_wid')\n",
    "del temp_shangxun_amtmean;gc.collect()\n",
    "#8月中旬交易均值\n",
    "temp_zhongxun_amtmean = df_trx[(df_trx['trx_d']<=20)&(df_trx['trx_d']>10)].groupby('cust_wid')['trx_amt'].mean().reset_index(drop=False)\n",
    "temp_zhongxun_amtmean.rename(columns = {'trx_amt':'zhongxun_trx_amt_mean'},inplace=True)\n",
    "#temp_zhongxun_amtsum.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_zhongxun_amtmean,how='left',on='cust_wid')\n",
    "del temp_zhongxun_amtmean;gc.collect()\n",
    "#8月下旬交易均值\n",
    "temp_xiaxun_amtmean = df_trx[df_trx['trx_d']>20].groupby('cust_wid')['trx_amt'].mean().reset_index(drop=False)\n",
    "temp_xiaxun_amtmean.rename(columns = {'trx_amt':'xiaxun_trx_amt_mean'},inplace=True)\n",
    "#temp_xiaxun_amtsum.fillna(0,inplace=True)\n",
    "trx_features= trx_features.merge(temp_xiaxun_amtmean,how='left',on='cust_wid')\n",
    "del temp_xiaxun_amtmean;gc.collect()\n",
    "#重要trx_cd计数\n",
    "temp = df_trx[df_trx['trx_cd'].isin(important_trx_cd)].groupby('cust_wid')['trx_cd'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_cd':'important_trx_cd_cnt'},inplace=True)\n",
    "trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "#重要trx_cd消费\n",
    "temp = df_trx[df_trx['trx_cd'].isin(important_trx_cd)].groupby('cust_wid')['trx_amt'].sum().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_amt':'important_amt_sum'},inplace=True)\n",
    "trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "#重要trx_cd消费\n",
    "temp = df_trx[df_trx['trx_cd'].isin(important_trx_cd)].groupby('cust_wid')['trx_amt'].mean().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_amt':'important_amt_mean'},inplace=True)\n",
    "trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "#重要trx_cd消费\n",
    "temp = df_trx[df_trx['trx_cd'].isin(important_trx_cd)].groupby('cust_wid')['trx_amt'].std().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_amt':'important_amt_std'},inplace=True)\n",
    "trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "\n",
    "\n",
    "trx_features[['shangxun_trx_count','zhongxun_trx_count','xiaxun_trx_count','shangxun_trx_amtsum',\n",
    "                  'zhongxun_trx_amt_sum','xiaxun_trx_amt_sum',\n",
    "                  'shangxun_trx_amtmean','zhongxun_trx_amt_mean','xiaxun_trx_amt_mean']] =trx_features[['shangxun_trx_count','zhongxun_trx_count','xiaxun_trx_count','shangxun_trx_amtsum',\n",
    "                  'zhongxun_trx_amt_sum','xiaxun_trx_amt_sum',\n",
    "                'shangxun_trx_amtmean','zhongxun_trx_amt_mean','xiaxun_trx_amt_mean']].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807ed25-292c-45f0-962b-6675661e77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_features.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3d515-03db-4a39-b5b2-d878fd5588a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f60c49-f72e-49fa-9e71-29ea9fbbc77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2339a2-09c3-4658-b277-c066bfaf0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个trx_cd进行了多少次交易\n",
    "temp_grouppage_count = df_trx.groupby('trx_cd')['trx_d'].count().reset_index(drop=False)\n",
    "temp_grouppage_count.rename(columns = {'trx_d':'group_cd_cnt'},inplace=True)\n",
    "df_trx = df_trx.merge(temp_grouppage_count,how='left',on='trx_cd')\n",
    "del temp_grouppage_count;gc.collect()\n",
    "#每个trx_cd进行了多少次交易的统计特征\n",
    "index = ['mean','std','max','min']\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'group_cd_cnt':f'group_cd_cnt_{ind}'},inplace=True)\n",
    "    trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "    \n",
    "\n",
    "#每个trx_cd被多少人交易\n",
    "temp_grouppage_uid_cnt = df_trx.groupby('trx_cd')['cust_wid'].nunique().reset_index(drop=False) \n",
    "temp_grouppage_uid_cnt.rename(columns = {'cust_wid':'group_cd_uid_cnt'},inplace=True)\n",
    "df_trx = df_trx.merge(temp_grouppage_uid_cnt,how='left',on='trx_cd')\n",
    "del temp_grouppage_uid_cnt;gc.collect()\n",
    "#每个trx_cd被多少人交易的统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_uid_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_uid_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_uid_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_uid_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'group_cd_uid_cnt':f'group_cd_uid_cnt_{ind}'},inplace=True)\n",
    "    trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#每个trx_cd在多少天被进行交易\n",
    "temp_grouppage_days_count = df_trx.groupby('trx_cd')['trx_d'].nunique().reset_index(drop=False)\n",
    "temp_grouppage_days_count.rename(columns = {'trx_d':'group_cd_days_cnt'},inplace=True)\n",
    "df_trx = df_trx.merge(temp_grouppage_days_count,how='left',on='trx_cd')\n",
    "del temp_grouppage_days_count;gc.collect()\n",
    "#每个trx_cd在多少天被进行交易的统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_days_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_days_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_days_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_trx.groupby('cust_wid')['group_cd_days_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'group_cd_days_cnt':f'group_cd_days_cnt_{ind}'},inplace=True)\n",
    "    trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#每个用户和trx_cd组合进行了多少次交易 \n",
    "temp = df_trx.groupby(['cust_wid','trx_cd'])['trx_d'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_d':'groupby_uid_cd_cnt'},inplace=True)\n",
    "df_trx = df_trx.merge(temp,how='left',on=['cust_wid','trx_cd'])\n",
    "del temp;gc.collect()\n",
    "#每个用户和trx_cd组合进行了多少次交易的统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_cd_cnt':f'groupby_uid_cd_cnt_{ind}'},inplace=True)\n",
    "    trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#每个用户和trx_cd组合在多少天进行了交易\n",
    "temp = df_trx.groupby(['cust_wid','trx_cd'])['trx_d'].nunique().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_d':'groupby_uid_cd_days_cnt'},inplace=True)\n",
    "df_trx = df_trx.merge(temp,how='left',on=['cust_wid','trx_cd'])\n",
    "del temp;gc.collect()\n",
    "#每个用户和trx_cd组合在多少天进行了交易的统计特征\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_days_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_days_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_days_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_cd_days_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_cd_days_cnt':f'groupby_uid_cd_days_cnt_{ind}'},inplace=True)\n",
    "    trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#每个用户在每天进行了多少次交易\n",
    "temp = df_trx.groupby(['cust_wid','trx_d'])['trx_cd'].count().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_cd':'groupby_uid_trxd_cnt'},inplace=True)\n",
    "df_trx = df_trx.merge(temp,how='left',on=['cust_wid','trx_d'])\n",
    "del temp;gc.collect()\n",
    "for ind in index:\n",
    "    if ind =='max':\n",
    "        break \n",
    "    if ind == 'mean':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_trxd_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_trxd_cnt'].std().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_trxd_cnt':f'groupby_uid_trxd_cnt_{ind}'},inplace=True)\n",
    "    trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()\n",
    "#每个用户在每天在多少交易通道进行了交易\n",
    "temp = df_trx.groupby(['cust_wid','trx_d'])['trx_cd'].nunique().reset_index(drop=False)\n",
    "temp.rename(columns = {'trx_cd':'groupby_uid_date_cd_cnt'},inplace=True)\n",
    "df_trx = df_trx.merge(temp,how='left',on=['cust_wid','trx_d'])\n",
    "del temp;gc.collect()\n",
    "for ind in index:\n",
    "    if ind == 'mean':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_date_cd_cnt'].mean().reset_index(drop=False)\n",
    "    elif ind=='std':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_date_cd_cnt'].std().reset_index(drop=False)\n",
    "    elif ind =='max':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_date_cd_cnt'].max().reset_index(drop=False)\n",
    "    elif ind =='min':\n",
    "        temp = df_trx.groupby('cust_wid')['groupby_uid_date_cd_cnt'].min().reset_index(drop=False)\n",
    "    temp.rename(columns = {'groupby_uid_date_cd_cnt':f'groupby_uid_date_cd_cnt_{ind}'},inplace=True)\n",
    "    trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "    del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123a367-9a22-4822-8451-365e0bbed351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529acad-2010-40b0-8b61-336c8034b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['sum','min','max','mean','std']\n",
    "#每个用户每一天交易了多少金额(脱敏)\n",
    "for i in range(1,32):\n",
    "    for ind in index:\n",
    "        if ind == 'mean':\n",
    "            temp = df_trx[df_trx['trx_d']==i].groupby('cust_wid')['trx_amt'].mean().reset_index(drop=False)\n",
    "        elif ind=='std':\n",
    "            temp = df_trx[df_trx['trx_d']==i].groupby('cust_wid')['trx_amt'].std().reset_index(drop=False)\n",
    "        elif ind =='max':\n",
    "            temp = df_trx[df_trx['trx_d']==i].groupby('cust_wid')['trx_amt'].max().reset_index(drop=False)\n",
    "        elif ind =='min':\n",
    "            temp = df_trx[df_trx['trx_d']==i].groupby('cust_wid')['trx_amt'].min().reset_index(drop=False)\n",
    "        elif ind =='sum':\n",
    "            temp = df_trx[df_trx['trx_d']==i].groupby('cust_wid')['trx_amt'].sum().reset_index(drop=False)\n",
    "        temp.rename(columns = {'trx_amt':f'day_{i}_amt_{ind}'},inplace=True)\n",
    "        trx_features = trx_features.merge(temp,how='left',on='cust_wid')\n",
    "        del temp;gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6951c7-69ad-4b06-8244-71e9a2cf9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trx.groupby('trx_d')['trx_amt'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d9394-d751-4112-a614-bc72afbfa48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b1ddc-6aab-4a03-97b8-a5040542b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_features.to_parquet('/work/trx_features_b.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff08e4-2e70-4597-9aa0-6c596c9d80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train_trx,df_test_trx;df_trx;trx_features;gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe98b4d2-633e-4887-b1be-00ce31e71bad",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  整合特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f409bf-3d45-40f7-9f4f-7b1a31ecba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_features = pd.read_parquet('view_features_b.parquet')\n",
    "trx_features = pd.read_parquet('trx_features_b.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d366d99-e523-42f5-938e-6f3ce53dd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.concat((df_train_base,df_test_base)) \n",
    "df = df_base.merge(view_features,how='left',on='cust_wid')\n",
    "df = df.merge(trx_features,how='left',on='cust_wid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d8cd75-3866-404b-b34f-bc6d461ad3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del view_features,trx_features;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cab9e9-e47c-4df5-aef1-1845bb49e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#后面发现年龄的特征重要性挺靠前,target_encoding特征重要性也靠前,在每个年龄对target_encoding做统计特征\n",
    "targetencoder_cols = ['page_id_target_mean','page_id_target_max','page_id_target_sum','page_id_target_min','page_id_target_std']\n",
    "index=['mean','std','max','min']\n",
    "for col in targetencoder_cols:\n",
    "    for ind in index:\n",
    "        if ind == 'mean':\n",
    "            temp = df.groupby('age')[col].mean().reset_index(drop=False)\n",
    "        elif ind=='std':\n",
    "            temp = df.groupby('age')[col].std().reset_index(drop=False)\n",
    "        elif ind =='max':\n",
    "            temp = df.groupby('age')[col].max().reset_index(drop=False)\n",
    "        elif ind =='min':\n",
    "            temp = df.groupby('age')[col].min().reset_index(drop=False)\n",
    "        temp.rename(columns = {col:f'group_age_{col}_{ind}'},inplace=True)\n",
    "        df = df.merge(temp,how='left',on='age')\n",
    "        del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c305cb2-917b-47eb-9a72-3d095bd50c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在每个年龄对交易量做统计特征\n",
    "targetencoder_cols = ['amt_max','amt_min','amt_all','amt_mean','amt_std']\n",
    "for col in targetencoder_cols:\n",
    "    for ind in index:\n",
    "        if ind == 'mean':\n",
    "            temp = df.groupby('age')[col].mean().reset_index(drop=False)\n",
    "        elif ind=='std':\n",
    "            temp = df.groupby('age')[col].std().reset_index(drop=False)\n",
    "        elif ind =='max':\n",
    "            temp = df.groupby('age')[col].max().reset_index(drop=False)\n",
    "        elif ind =='min':\n",
    "            temp = df.groupby('age')[col].min().reset_index(drop=False)\n",
    "        temp.rename(columns = {col:f'group_age_{col}_{ind}'},inplace=True)\n",
    "        df = df.merge(temp,how='left',on='age')\n",
    "        del temp;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced02e6-c08c-4055-a1a5-0f150939963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cty_cd_num'] = pd.to_numeric(df['cty_cd'], errors='coerce') \n",
    "df['cty_cd_cat'] = df['cty_cd'].apply(lambda x: x if x in ['A', 'B', 'C'] else 'D' if pd.notna(x) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499d5c8-5660-4b6a-9f03-404ae0a76f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#我对cty_cd的理解有误把邮编当作数值特征了,然后发现值非常大取了个对数,没软用,请无视\n",
    "md_cty_cd_num = df['cty_cd_num'].median()\n",
    "df['cty_cd_num'] =df['cty_cd_num'].fillna(md_cty_cd_num)\n",
    "def log_nonzero(x):\n",
    "    if x == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.log(x)\n",
    "# 计算最小非零值的对数\n",
    "min_log = np.log(df[df['cty_cd_num'] != 0]['cty_cd_num'].min())\n",
    "# 对 A 列进行操作\n",
    "df['log_cty_cd_num'] = df['cty_cd_num'].apply(lambda x: log_nonzero(x) if x != np.nan else np.nan)\n",
    "df['log_cty_cd_num'] = df['log_cty_cd_num'].fillna(min_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b3617-746b-48d7-8128-7b9b766b9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#之前对年龄做了描述性统计,发现年龄居然有90+的,一眼假,因此对年龄进行了等距分箱,如下\n",
    "df['age'].replace(-1, np.nan, inplace=True)\n",
    "def get_age_bins(x):\n",
    "    if x<=20:\n",
    "        return 1\n",
    "    elif 20<x<=30:\n",
    "        return 2 \n",
    "    elif 30<x<=40:\n",
    "        return 3\n",
    "    elif 40<x<=50:\n",
    "        return 4\n",
    "    elif 50<x<=60:\n",
    "        return 5\n",
    "    elif 60<x<=70:\n",
    "        return 6\n",
    "    elif x>70:\n",
    "        return 7\n",
    "df['age_bins'] = df['age'].apply(get_age_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3f4db-ad02-4fef-8f4f-0a178a90079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#年龄分箱均值特征,取的特征也比较随意\n",
    "age_bins_group_columns = ['age_bins','cty_cd_num','view_count', 'page_count', 'acs_days_count',\n",
    "       'shangxun_view_count', 'zhongxun_view_count', 'xiaxun_view_count',\n",
    "       'trx_count', 'trx_cd_count', 'trx_days_count', 'shangxun_trx_count',\n",
    "       'zhongxun_trx_count', 'xiaxun_trx_count', 'amt_all', 'amt_mean',\n",
    "       'amt_std', 'amt_max', 'amt_min', 'shangxun_trx_amtsum',\n",
    "       'zhongxun_trx_amt_sum', 'xiaxun_trx_amt_sum', 'shangxun_trx_amtmean',\n",
    "       'zhongxun_trx_amt_mean', 'xiaxun_trx_amt_mean', 'log_cty_cd_num']\n",
    "# age_bins_group_columns = list(set(df.columns)-set(['age_bins','cust_wid', 'label', 'age', 'gdr_cd', 'cty_cd', 'cty_cd_num',\n",
    "#        'cty_cd_cat']))\n",
    "# age_bins_group_columns =['age_bins']+age_bins_group_columns\n",
    "temp_group_agebins_feature = df[age_bins_group_columns].groupby('age_bins').mean().reset_index(drop=False)\n",
    "group_agebins_colnames = [ age_bins_group_columns[i]+'group_ages' for i in range(1,len(age_bins_group_columns))]\n",
    "temp_group_agebins_feature.columns =['age_bins']+group_agebins_colnames \n",
    "df =df.merge(temp_group_agebins_feature,how='left',on='age_bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24e522-c398-4641-8f5e-a8a7912787d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0591a84-79f7-464c-90fe-9e5bce462cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把不纳入特征的字段去掉\n",
    "useless_cols = ['cust_wid','label','cty_cd']\n",
    "all_cols = list(df.columns)\n",
    "use_cols = list(set(all_cols) - set(useless_cols))\n",
    "#离散特征,树模型无法直接识别,需要做编码,这里用简单的labelEncoder\n",
    "cate_feature = ['gdr_cd','cty_cd_cat','age_bins']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for item in cate_feature:\n",
    "    df[item] = LabelEncoder().fit_transform(df[item])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d10d5e1-445b-44c7-acc6-24a8eff4d7e0",
   "metadata": {},
   "source": [
    "## 特征筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc26156-d7c2-401a-8912-8afa75bc66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征构造结束了,因为我对page_id做了很多的特征,现在特征有一两千个了,需要做筛选\n",
    "#构造一个简单的LGB模型,输出特征重要性\n",
    "import lightgbm as lgb\n",
    "params_clf = {'num_leaves': 60,\n",
    "          'min_data_in_leaf': 30,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.1,\n",
    "          \"min_sum_hessian_in_leaf\": 6,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"feature_fraction\": 0.9,\n",
    "          \"bagging_freq\": 1,\n",
    "          \"bagging_fraction\": 0.8,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"lambda_l1\": 0.1,\n",
    "          #'lambda_l2': 0.2,\n",
    "          \"verbosity\": -1,\n",
    "          \"nthread\": -1,\n",
    "          'metric': {'binary_logloss', 'auc'},\n",
    "          \"random_state\": 2019,\n",
    "          # 'device': 'gpu'\n",
    "          }\n",
    "train_data = df[~df['label'].isnull()]\n",
    "train_X = train_data[use_cols]\n",
    "train_Y = train_data['label'].apply(lambda x: 0 if x == 0 else 1)\n",
    "trn_data = lgb.Dataset(train_X, label=train_Y)\n",
    "clf = lgb.train(params_clf,\n",
    "                trn_data,\n",
    "                100,\n",
    "                verbose_eval=-1,\n",
    "                categorical_feature=cate_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18799437-0c30-4614-8d6a-44b2af7d028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取特征重要性\n",
    "importance = clf.feature_importance()\n",
    "\n",
    "# 将特征重要性排序并输出\n",
    "indices = importance.argsort()[::-1]\n",
    "for f in range(train_X.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, train_X.columns[indices[f]], importance[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276f7ac-dd3a-483e-a0fc-fff1624e3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出特征重要性大于0的特征,也就是在lgb模型有取出这些特征作为分裂结点依据\n",
    "important_features = train_X.columns[importance > 0]\n",
    "len(important_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e85e0dab",
   "metadata": {},
   "source": [
    "# 训练与预测"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec6e815",
   "metadata": {},
   "source": [
    "## 第一阶段:二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7510d4b-caeb-4c16-bf42-192a6655f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from tqdm import tqdm\n",
    "#定义线下选择最优阈值的函数\n",
    "def find_best_threshold(y_valid, oof_prob):\n",
    "    best_f2 = 0\n",
    "    ths=[]\n",
    "    f2s=[]\n",
    "    for th in tqdm([i/1000 for i in range(0, 200)]):\n",
    "        oof_prob_copy = oof_prob.copy()\n",
    "        oof_prob_copy[oof_prob_copy >= th] = 1\n",
    "        oof_prob_copy[oof_prob_copy < th] = 0\n",
    "\n",
    "        # recall = recall_score(y_valid, oof_prob_copy)\n",
    "        # precision = precision_score(y_valid, oof_prob_copy)\n",
    "        # f1 = 2*recall*precision / (precision+recall)\n",
    "        f2 = fbeta_score(y_valid, oof_prob_copy, beta=2.0)\n",
    "        ths.append(th)\n",
    "        f2s.append(f2)\n",
    "        if f2 > best_f2:\n",
    "            best_th = th\n",
    "            best_f2 = f2\n",
    "        gc.collect()\n",
    "        \n",
    "    return best_th, best_f2, ths,f2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00afa3-ab70-4489-bd53-158ed6dc60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#五折交叉验证的lgb模型\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "\n",
    "train_data = df[~df['label'].isnull()]\n",
    "train_X = train_data[important_features]\n",
    "#train_X = train_data[use_cols]\n",
    "train_Y = train_data['label'].apply(lambda x: 0 if x == 0 else 1)\n",
    "test = df[df['label'].isnull()][important_features]\n",
    "#test = df[df['label'].isnull()][use_cols]\n",
    "five_fold_pre =pd.DataFrame(index=range(50000)) \n",
    "test_pred_prob = np.zeros((test.shape[0], ))\n",
    "cate_feature = ['gdr_cd', 'cty_cd_cat']\n",
    "#cate_feature = ['gdr_cd']\n",
    "params_clf = {'num_leaves': 60,\n",
    "          'min_data_in_leaf': 30,\n",
    "          'objective': 'binary',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.1,\n",
    "          \"min_sum_hessian_in_leaf\": 6,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"feature_fraction\": 0.9,\n",
    "          \"bagging_freq\": 1,\n",
    "          \"bagging_fraction\": 0.8,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"lambda_l1\": 0.1,\n",
    "          'lambda_l2': 0.2,\n",
    "          \"verbosity\": -1,\n",
    "          \"nthread\": -1,\n",
    "          'metric': {'binary_logloss', 'auc'},\n",
    "          \"random_state\": 2019,\n",
    "          # 'device': 'gpu'\n",
    "          }\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "prob_oof = np.zeros((train_X.shape[0], ))\n",
    "test_pred_prob = np.zeros((test.shape[0], ))\n",
    "## train and predict\n",
    "feature_importance_df = pd.DataFrame()\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data,y=train_data['label'])):\n",
    "    print(\"fold {}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(train_X.iloc[trn_idx], label=train_Y[trn_idx])\n",
    "    val_data = lgb.Dataset(train_X.iloc[val_idx], label=train_Y[val_idx])\n",
    "    clf = lgb.train(params_clf,\n",
    "                    trn_data,\n",
    "                    2000,\n",
    "                    valid_sets=[trn_data, val_data],\n",
    "                    verbose_eval=-1,\n",
    "                    categorical_feature=cate_feature,\n",
    "                    early_stopping_rounds=100)\n",
    "    prob_oof[val_idx] = clf.predict(train_X.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = important_features\n",
    "    #fold_importance_df[\"Feature\"] = use_cols\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    pre = clf.predict(test, num_iteration=clf.best_iteration) \n",
    "    test_pred_prob += pre / folds.n_splits\n",
    "    five_fold_pre[f'fold_{fold_}'] = pre \n",
    "    plt.hist(clf.predict(test, num_iteration=clf.best_iteration))\n",
    "    plt.show()\n",
    "    valid_pred_prob =  clf.predict(train_X.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    best_th, best_f2, ths,f2s = find_best_threshold(train_Y[val_idx], valid_pred_prob)\n",
    "    print(best_th, best_f2)\n",
    "    plt.plot(ths,f2s)\n",
    "    plt.show()\n",
    "    pickle.dump(clf,open(f'/work/test_data/lgb_clf_{fold_}_date_5.pkl', 'wb'))\n",
    "# clf = lgb.train(params_clf,\n",
    "#                 trn_data,\n",
    "#                 10000,\n",
    "#                 valid_sets=[trn_data, val_data],\n",
    "#                 verbose_eval=-1,\n",
    "#                 categorical_feature=cate_feature,\n",
    "#                 early_stopping_rounds=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19ca79-def9-4909-81b3-8958589f9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#线下训练集选最优阈值,可惜没啥软用,这里我是选出了绝对阈值,也可以选相对阈值,也就是阈值在所有预测分数的分位数\n",
    "best_th, best_f2, ths,f2s = find_best_threshold(train_Y, prob_oof)\n",
    "print(best_th, best_f2)\n",
    "plt.plot(ths,f2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13716d18-3135-433d-99d6-0c79bedb5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e4a21-e05f-4f7e-8512-fbf8019db188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#五折的XGB\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "five_fold_pre =pd.DataFrame(index=range(50000)) \n",
    "prob_oof = np.zeros((train_X.shape[0], ))\n",
    "test_pred_prob = np.zeros((test.shape[0], ))\n",
    "## train and predict\n",
    "#feature_importance_df = pd.DataFrame()\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data,y=train_data['label'])):\n",
    "    print(\"fold {}\".format(fold_ + 1))\n",
    "    #trn_data = lgb.Dataset(train_X.iloc[trn_idx], label=train_Y[trn_idx])\n",
    "    #val_data = lgb.Dataset(train_X.iloc[val_idx], label=train_Y[val_idx])\n",
    "    clf = XGBClassifier(learning_rate=0.1,\n",
    "                n_estimators=2000,\n",
    "                min_child_weight=5,\n",
    "                max_delta_step=0,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                silent=True,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='auc',\n",
    "                seed=1440,\n",
    "                gamma=0,\n",
    "                early_stopping_rounds=50)\n",
    "    clf.fit(train_X.iloc[trn_idx],train_Y[trn_idx],eval_set=[(train_X.iloc[val_idx], train_Y[val_idx])])\n",
    "    prob_oof[val_idx] = clf.predict_proba(train_X.iloc[val_idx])[:,1]\n",
    "    \n",
    "    #fold_importance_df = pd.DataFrame()\n",
    "    #fold_importance_df[\"Feature\"] = important_features\n",
    "    #fold_importance_df[\"Feature\"] = use_cols\n",
    "    #fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    #fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    pre = clf.predict_proba(test)[:,1]\n",
    "    test_pred_prob += pre / folds.n_splits\n",
    "    five_fold_pre[f'fold_{fold_}'] = pre \n",
    "    plt.hist(pre)\n",
    "    plt.show()\n",
    "    valid_pred_prob =  clf.predict_proba(train_X.iloc[val_idx])[:,1]\n",
    "    best_th, best_f2, ths,f2s = find_best_threshold(train_Y[val_idx], valid_pred_prob)\n",
    "    print(best_th, best_f2)\n",
    "    plt.plot(ths,f2s)\n",
    "    plt.show()\n",
    "    pickle.dump(clf,open(f'/work/test_data/xgb_clf_{fold_}_date_5.pkl', 'wb'))\n",
    "# clf = lgb.train(params_clf,\n",
    "#                 trn_data,\n",
    "#                 10000,\n",
    "#                 valid_sets=[trn_data, val_data],\n",
    "#                 verbose_eval=-1,\n",
    "#                 categorical_feature=cate_feature,\n",
    "#                 early_stopping_rounds=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85901396-564f-4fd7-ba7d-675e68a8de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "five_fold_pre =pd.DataFrame(index=range(50000)) \n",
    "prob_oof = np.zeros((train_X.shape[0], ))\n",
    "test_pred_prob = np.zeros((test.shape[0], ))\n",
    "## train and predict\n",
    "#feature_importance_df = pd.DataFrame()\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data,y=train_data['label'])):\n",
    "    print(\"fold {}\".format(fold_ + 1))\n",
    "    #trn_data = lgb.Dataset(train_X.iloc[trn_idx], label=train_Y[trn_idx])\n",
    "    #val_data = lgb.Dataset(train_X.iloc[val_idx], label=train_Y[val_idx])\n",
    "    clf = CatBoostClassifier(iterations=10000,\n",
    "                             learning_rate=0.01,\n",
    "                             l2_leaf_reg=0.1,\n",
    "                             verbose=10,\n",
    "                             early_stopping_rounds=400,\n",
    "                             eval_metric='AUC')\n",
    "    clf.fit(train_X.iloc[trn_idx],train_Y[trn_idx],eval_set=[(train_X.iloc[val_idx], train_Y[val_idx])])\n",
    "    prob_oof[val_idx] = clf.predict_proba(train_X.iloc[val_idx])[:,1]\n",
    "    \n",
    "    #fold_importance_df = pd.DataFrame()\n",
    "    #fold_importance_df[\"Feature\"] = important_features\n",
    "    #fold_importance_df[\"Feature\"] = use_cols\n",
    "    #fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    #fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    pre = clf.predict_proba(test)[:,1]\n",
    "    test_pred_prob += pre / folds.n_splits\n",
    "    five_fold_pre[f'fold_{fold_}'] = pre \n",
    "    plt.hist(pre)\n",
    "    plt.show()\n",
    "    valid_pred_prob =  clf.predict_proba(train_X.iloc[val_idx])[:,1]\n",
    "    best_th, best_f2, ths,f2s = find_best_threshold(train_Y[val_idx], valid_pred_prob)\n",
    "    print(best_th, best_f2)\n",
    "    plt.plot(ths,f2s)\n",
    "    plt.show()\n",
    "    pickle.dump(clf,open(f'/work/test_data/cb_clf_{fold_}_date_5.pkl', 'wb'))\n",
    "# clf = lgb.train(params_clf,\n",
    "#                 trn_data,\n",
    "#                 10000,\n",
    "#                 valid_sets=[trn_data, val_data],\n",
    "#                 verbose_eval=-1,\n",
    "#                 categorical_feature=cate_feature,\n",
    "#                 early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a101566-ee59-4102-a2be-abb3411ef4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a39582-1c89-44b6-9cb1-69f225d0da64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fe406-d44c-4202-8565-351a5485a523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1f201-abff-47fe-bdf9-8614a93a586f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c90119-8077-4f38-9ec5-6a5b683c37d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46caad02-61e5-497f-a622-3d939a06561f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d0afb-a137-4627-8c43-5cf47952bd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfcba2-9cea-4581-a107-1fe3dbc1f161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02110d8b-2227-4be9-97e1-1f49faae9cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b39dd9-94a2-40f9-bc7e-bda61c21d614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2827f-10c1-49cb-a3d5-0f6b31840d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760ebad-678a-4dde-810d-a0e9423fd418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713682b-6df8-4c21-8564-1e62c82f3c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b0a5d-16f2-47e1-baf9-819b3ed3cf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b8fc0-9941-4aa5-ab52-c5555331e8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be40e7b-17d0-461f-bfff-b71f615428a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c200294-2659-413d-8189-3e6a2b94afe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05e47e-8edf-463d-bb41-fd71ef0eb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prob_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d58cc-7edf-4e6d-b024-85f8a7c45f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98616ae-6357-4283-b75c-44c40c8f1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3701b98-aef8-4e26-942e-27b602051585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集的特征\n",
    "test_X = df[df['label'].isnull()][important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc5d47-4f00-47d8-adbe-c44b4d6e6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集预测过程,这里二分类要卡一个阈值,如果按照线下的分位数去卡阈值,其实不是最优的,经过尝试(猜),我确定了0.08这个绝对阈值,换作相对阈值大概是0比1=34比36\n",
    "test_label_T = pd.DataFrame(df[df['label'].isnull()]['cust_wid'])\n",
    "test_label_T['pre_prob'] = test_pred_prob\n",
    "test_label_T['two_class_label'] = test_label_T['pre_prob'].apply(lambda x:1 if x>0.08 else 0)\n",
    "test_label_T = pd.concat((test_label_T,test_X),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c46a64f-27b2-4f44-bfd3-c9a0ee24f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_label_T['pre_prob'].apply(lambda x:1 if x>0.175 else 0).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66ebad-5842-42d4-9c5e-d36a4c5224bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49385933-6ef2-4495-a10b-86ab4347028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['label'].isnull()][important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106351e6-367c-4b05-9076-ceba731fb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_reg(y_valid, oof_prob):\n",
    "    best_mape = 100000\n",
    "    ths=[]\n",
    "    mapes=[]\n",
    "    for th in tqdm([1+i/100 for i in range(50, 100)]):\n",
    "        oof_prob_copy = oof_prob.copy()\n",
    "        oof_prob_copy[oof_prob_copy >= th] = 2\n",
    "        oof_prob_copy[oof_prob_copy < th] = 1\n",
    "        \n",
    "        # recall = recall_score(y_valid, oof_prob_copy)\n",
    "        # precision = precision_score(y_valid, oof_prob_copy)\n",
    "        # f1 = 2*recall*precision / (precision+recall)\n",
    "        mape = np.mean(np.abs(oof_prob_copy-y_valid)/y_valid)\n",
    "        ths.append(th)\n",
    "        mapes.append(mape)\n",
    "        if mape < best_mape:\n",
    "            best_th = th\n",
    "            best_mape = mape\n",
    "        gc.collect()\n",
    "        \n",
    "    return best_th, best_mape, ths,mapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524ce3a-0b60-48bb-a3cb-4ddd965908b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['label']>0].reset_index(drop=True )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7be310ca-970d-4c32-a90e-80baa1b5d10d",
   "metadata": {},
   "source": [
    "## 第二阶段：回归任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d62ae-6e17-4938-a8ee-45e55aeba618",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2022)\n",
    "#回归阶段训练数据选取训练集里label>0 即1-14的样本\n",
    "train_reg_data = df[df['label']>0].reset_index(drop=True )\n",
    "pre_reg_oof = np.zeros((train_reg_data.shape[0], ))\n",
    "test_pred_reg = np.zeros((test.shape[0], ))\n",
    "train_X = train_reg_data[important_features]\n",
    "train_Y = train_reg_data['label']\n",
    "## train and predict\n",
    "feature_importance_df = pd.DataFrame()\n",
    "params_reg = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'mape',\n",
    "            'metric':'mape',\n",
    "            'max_depth': -1,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "#五折LGB回归用MAPE做损失\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_reg_data,y=train_reg_data['label'])):\n",
    "    print(\"fold {}\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(train_X.iloc[trn_idx], label=train_Y[trn_idx])\n",
    "    val_data = lgb.Dataset(train_X.iloc[val_idx], label=train_Y[val_idx])\n",
    "    reg = lgb.train(params_reg,\n",
    "                    trn_data,\n",
    "                    2000,\n",
    "                    valid_sets=[trn_data, val_data],\n",
    "                    verbose_eval=-1,\n",
    "                    categorical_feature=cate_feature,\n",
    "                    early_stopping_rounds=100)\n",
    "    pre_reg_oof[val_idx] = reg.predict(train_X.iloc[val_idx], num_iteration=reg.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = important_features\n",
    "    #fold_importance_df[\"Feature\"] = use_cols\n",
    "    fold_importance_df[\"importance\"] = reg.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    pre = reg.predict(test, num_iteration=reg.best_iteration) \n",
    "    test_pred_reg += pre / folds.n_splits\n",
    "    five_fold_pre[f'fold_{fold_}'] = pre \n",
    "\n",
    "    valid_pred_reg =  reg.predict(train_X.iloc[val_idx], num_iteration=reg.best_iteration)\n",
    "    best_th, best_mape, ths,mapes = find_best_threshold_reg(train_Y[val_idx], valid_pred_reg)\n",
    "    print(best_th, best_mape)\n",
    "    plt.plot(ths,mapes)\n",
    "    plt.show()\n",
    "    pickle.dump(reg,open(f'/work/model_data/lgb_reg_{fold_}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de7a14-cad8-497e-aef2-64bf211d3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_fold_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059e7ba-d6fb-472c-b0d2-5497eeccfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#线下回归阈值选择,用MAPE损失可以发现训练集预测的值都在2.5以下,所以只需要选一个阈值把预测结果分为1和2两类\n",
    "best_th, best_mape, ths,mapes = find_best_threshold_reg(train_Y, pre_reg_oof)\n",
    "print(best_th, best_mape)\n",
    "plt.plot(ths,mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a76df-78c0-4cf0-ad47-dfc8f2e8b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(pre_reg_oof),np.max(pre_reg_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cc519-a290-4fe4-9785-72b244323bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(test_pred_reg),np.max(test_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07168f49-fe3b-480f-9f84-feab96ad9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test_pred_reg<1.93)/len(test_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16487e24-f69b-4a8e-a4ec-bf2d1d04bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pre_reg_oof>1.69)#线下为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad9f31-2b59-4824-89e3-d2f281954e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pre_reg_oof<1.69)#线下为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48634d45-cb68-4afa-b924-1bf7027e4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#单模的lgb回归,其实和五折效果差不多吧,所以没把代码删掉\n",
    "params_reg = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'mape',\n",
    "            'metric':'mape',\n",
    "            'max_depth': -1,\n",
    "            'num_leaves': 100,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "train_data, val_data = train_test_split(df[df['label']>0], test_size=0.2, random_state=42)\n",
    "train_X = train_data[important_features]\n",
    "train_Y = train_data['label']\n",
    "val_X = val_data[important_features]\n",
    "val_Y = val_data['label']\n",
    "trn_data = lgb.Dataset(train_X, label=train_Y)\n",
    "val_data = lgb.Dataset(val_X, label=val_Y)\n",
    "callback=[lgb.early_stopping(stopping_rounds=100,verbose=True),\n",
    "          lgb.log_evaluation(period=10,show_stdv=True)]\n",
    "reg = lgb.train(params_reg,trn_data,num_boost_round=2000,\n",
    "                valid_sets=val_data,callbacks=callback)\n",
    "pickle.dump(reg,open(f'/work/test_data/lgb_reg.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b54bb6-712c-4ac4-b9ef-a8177be16dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#单模阈值选择\n",
    "best_th, best_mape, ths,mapes = find_best_threshold_reg(val_Y,val_reg_pre)\n",
    "print(best_th,best_mape)\n",
    "plt.plot(ths,mapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3975a-9218-4b17-b5bc-1ffa3dc79b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#回归阶段的预测,这里还没用阈值进行分类\n",
    "test_reg_X = test_label_T[test_label_T['two_class_label']==1][important_features]\n",
    "reg_pre = reg.predict(test_reg_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc5e1d-2b5d-444e-b65d-6155858ecc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead1ea4-411b-4f42-8425-ba09c5ccfd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e948dcf-20fe-415e-9987-4e9cdcf735e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把测试集的特征存在本地\n",
    "test_label_T.to_parquet('/work/model_data/test_label_T.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e866ccc-d5c8-455e-92fe-b60b4fb0c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(important_features) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29bd2af5-9d22-49c5-a6e5-9962f03443fb",
   "metadata": {},
   "source": [
    "# 离线任务代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4f0f9-a1d6-401f-9039-09187da7b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "test_T = pd.read_parquet('test_data/test_label_T.parquet')\n",
    "lgb_reg = pickle.load(open('test_data/lgb_reg.pkl', 'rb'))\n",
    "#all_cols = list(test_T.columns )\n",
    "#useless_cols = ['cust_wid','pre_prob','two_class_label']\n",
    "use_cols = ['date_nan_pagetargetencode_mean',\n",
    " 'page_300SL_cnt',\n",
    " 'groupby_uid_date_cd_cnt_max',\n",
    " 'date_2.0_page_nums_std',\n",
    " 'page_E41C0_cnt',\n",
    " 'date_23.0_page_nums_max',\n",
    " 'day_11_amt_std',\n",
    " 'groupby_uid_pid_cnt_max',\n",
    " 'day_30_amt_mean',\n",
    " 'page_0A4YS_cnt',\n",
    " 'date_9.0_page_nums_min',\n",
    " 'groupby_uid_trxd_cnt_std',\n",
    " 'date_4.0_pagetargetencode_mean',\n",
    " 'group_age_amt_max_max',\n",
    " 'date_16.0_page_nums_sum',\n",
    " 'xiaxun_trx_count',\n",
    " 'date_30.0_pagetargetencode_std',\n",
    " 'day_4_amt_mean',\n",
    " 'day_1_amt_sum',\n",
    " 'date_9.0_page_nums_mean',\n",
    " 'shangxun_view_count',\n",
    " 'group_age_page_id_target_sum_mean_y',\n",
    " 'date_2.0_pagetargetencode_mean',\n",
    " 'day_20_amt_std',\n",
    " 'cty_cd_cat',\n",
    " 'page_223AS_cnt',\n",
    " 'day_25_amt_mean',\n",
    " 'day_12_amt_sum',\n",
    " 'group_age_page_id_target_std_max_x',\n",
    " 'zhongxun_trx_amt_mean',\n",
    " 'date_23.0_page_nums_sum',\n",
    " 'day_16_amt_max',\n",
    " 'date_14.0_page_nums_mean',\n",
    " 'date_17.0_pagetargetencode_std',\n",
    " 'date_10.0_page_nums_min',\n",
    " 'zhongxun_trx_countgroup_ages',\n",
    " 'day_7_amt_min',\n",
    " 'date_nan_pagetargetencode_max',\n",
    " 'group_age_page_id_target_mean_mean_y',\n",
    " 'important_page_ndays_x',\n",
    " 'date_11.0_page_nums_std',\n",
    " 'date_22.0_pagetargetencode_min',\n",
    " 'day_27_amt_sum',\n",
    " 'day_21_amt_sum',\n",
    " 'day_11_amt_sum',\n",
    " 'date_5.0_page_nums_std',\n",
    " 'day_11_amt_min',\n",
    " 'date_23.0_pagetargetencode_min',\n",
    " 'page_F80RP_cnt',\n",
    " 'shangxun_trx_amtmeangroup_ages',\n",
    " 'groupby_uid_pid_cnt_mean',\n",
    " 'page_8C6XP_cnt',\n",
    " 'date_nan_pagetargetencode_min',\n",
    " 'day_22_amt_max',\n",
    " 'day_6_amt_max',\n",
    " 'day_7_amt_sum',\n",
    " 'date_4.0_pagetargetencode_sum',\n",
    " 'group_age_page_id_target_std_min_x',\n",
    " 'page_0A1LE_cnt',\n",
    " 'date_6.0_pagetargetencode_max',\n",
    " 'xiaxun_page_id_target_sum',\n",
    " 'day_8_amt_min',\n",
    " 'day_31_amt_mean',\n",
    " 'day_10_amt_sum',\n",
    " 'age',\n",
    " 'day_17_amt_mean',\n",
    " 'zhongxun_trx_amt_sum',\n",
    " 'page_77CSF_cnt',\n",
    " 'zhongxun_trx_count',\n",
    " 'day_12_amt_std',\n",
    " 'group_age_page_id_target_sum_std_x',\n",
    " 'shangxun_page_id_target_sum',\n",
    " 'page_CECAX_cnt',\n",
    " 'day_4_amt_min',\n",
    " 'group_age_page_id_target_min_max_x',\n",
    " 'group_cd_cnt_min',\n",
    " 'date_18.0_pagetargetencode_sum',\n",
    " 'max_oneday_views',\n",
    " 'page_005LA_cnt',\n",
    " 'page_CACZQ_cnt',\n",
    " 'date_15.0_pagetargetencode_max',\n",
    " 'day_22_amt_sum',\n",
    " 'day_4_amt_sum',\n",
    " 'groupby_uid_date_cnt_mean',\n",
    " 'day_20_amt_mean',\n",
    " 'page_677JS_cnt',\n",
    " 'day_19_amt_sum',\n",
    " 'day_18_amt_min',\n",
    " 'day_26_amt_mean',\n",
    " 'page_ECEBC_cnt',\n",
    " 'day_7_amt_std',\n",
    " 'group_cd_cnt_mean',\n",
    " 'page_F36Z4_cnt',\n",
    " 'page_D74FS_cnt',\n",
    " 'page_7B6KD_cnt',\n",
    " 'day_28_amt_sum',\n",
    " 'day_17_amt_max',\n",
    " 'page_4A2C4_cnt',\n",
    " 'day_5_amt_max',\n",
    " 'page_48CAR_cnt',\n",
    " 'page_995EA_cnt',\n",
    " 'groupby_uid_cd_cnt_mean',\n",
    " 'day_16_amt_sum',\n",
    " 'day_31_amt_max',\n",
    " 'day_15_amt_mean',\n",
    " 'day_22_amt_min',\n",
    " 'group_age_page_id_target_min_mean_y',\n",
    " 'trx_cd_count',\n",
    " 'page_409GD_cnt',\n",
    " 'page_4DEAO_cnt',\n",
    " 'xiaxun_trx_amt_meangroup_ages',\n",
    " 'page_FDBPE_cnt',\n",
    " 'groupby_uid_date_page_cnt_min',\n",
    " 'day_3_amt_mean',\n",
    " 'page_8D6ND_cnt',\n",
    " 'day_5_amt_std',\n",
    " 'page_43CTK_cnt',\n",
    " 'day_30_amt_std',\n",
    " 'day_15_amt_min',\n",
    " 'day_4_amt_std',\n",
    " 'xiaxun_trx_amt_sum',\n",
    " 'page_9AAZX_cnt',\n",
    " 'day_21_amt_std',\n",
    " 'day_5_amt_mean',\n",
    " 'groupby_uid_date_cnt_std',\n",
    " 'groupby_uid_date_page_cnt_mean',\n",
    " 'day_22_amt_mean',\n",
    " 'day_23_amt_min',\n",
    " 'page_B1EXB_cnt',\n",
    " 'day_15_amt_std',\n",
    " 'xiaxun_page',\n",
    " 'group_page_uid_cnt_min',\n",
    " 'group_page_days_cnt_mean',\n",
    " 'group_age_amt_max_min',\n",
    " 'page_CD0CJ_cnt',\n",
    " 'amt_maxgroup_ages',\n",
    " 'day_21_amt_min',\n",
    " 'day_23_amt_mean',\n",
    " 'page_974RN_cnt',\n",
    " 'day_13_amt_mean',\n",
    " 'day_14_amt_max',\n",
    " 'group_age_page_id_target_std_mean_x',\n",
    " 'day_19_amt_min',\n",
    " 'group_age_page_id_target_std_mean_y',\n",
    " 'page_663YB_cnt',\n",
    " 'page_4D5FF_cnt',\n",
    " 'groupby_uid_trxd_cnt_mean',\n",
    " 'page_1B4LA_cnt',\n",
    " 'page_A19CA_cnt',\n",
    " 'page_ACBYT_cnt',\n",
    " 'day_3_amt_min',\n",
    " 'day_2_amt_sum',\n",
    " 'amt_min',\n",
    " 'day_14_amt_sum',\n",
    " 'day_17_amt_sum',\n",
    " 'page_7B0KB_cnt',\n",
    " 'day_3_amt_std',\n",
    " 'page_3DCEZ_cnt',\n",
    " 'page_DC9DA_cnt',\n",
    " 'day_8_amt_sum',\n",
    " 'day_21_amt_max',\n",
    " 'zhongxun_page_id_target_mean',\n",
    " 'page_97BRS_cnt',\n",
    " 'page_A5812_cnt',\n",
    " 'page_51DDS_cnt',\n",
    " 'page_43EQW_cnt',\n",
    " 'day_10_amt_max',\n",
    " 'group_age_amt_std_mean',\n",
    " 'group_age_amt_min_mean',\n",
    " 'day_26_amt_std',\n",
    " 'page_1E4CI_cnt',\n",
    " 'groupby_uid_cd_cnt_max',\n",
    " 'page_1D6AN_cnt',\n",
    " 'shangxun_page',\n",
    " 'amt_std',\n",
    " 'day_31_amt_min',\n",
    " 'page_194IA_cnt',\n",
    " 'day_1_amt_min',\n",
    " 'group_cd_uid_cnt_mean',\n",
    " 'day_15_amt_sum',\n",
    " 'day_2_amt_min',\n",
    " 'group_age_amt_max_std',\n",
    " 'zhongxun_page_id_target_min',\n",
    " 'page_8E0ST_cnt',\n",
    " 'page_99DTC_cnt',\n",
    " 'page_808II_cnt',\n",
    " 'day_23_amt_sum',\n",
    " 'day_20_amt_sum',\n",
    " 'group_age_amt_std_max',\n",
    " 'groupby_uid_pid_days_cnt_std',\n",
    " 'group_age_page_id_target_sum_max_y',\n",
    " 'page_F50YG_cnt',\n",
    " 'page_787JY_cnt',\n",
    " 'day_19_amt_mean',\n",
    " 'day_9_amt_max',\n",
    " 'day_9_amt_mean',\n",
    " 'trx_days_count',\n",
    " 'group_age_page_id_target_mean_std_x',\n",
    " 'page_765RW_cnt',\n",
    " 'day_27_amt_min',\n",
    " 'day_30_amt_min',\n",
    " 'page_2F0LB_cnt',\n",
    " 'important_amt_mean',\n",
    " 'xiaxun_page_id_target_min',\n",
    " 'group_age_amt_mean_min',\n",
    " 'page_7BCJH_cnt',\n",
    " 'group_page_uid_cnt_mean',\n",
    " 'amt_mean',\n",
    " 'day_29_amt_mean',\n",
    " 'page_BB0QB_cnt',\n",
    " 'important_trx_cd_cnt',\n",
    " 'page_982KV_cnt',\n",
    " 'xiaxun_trx_amt_mean',\n",
    " 'day_14_amt_mean',\n",
    " 'day_6_amt_min',\n",
    " 'page_E56CC_cnt',\n",
    " 'page_F3CFD_cnt',\n",
    " 'day_2_amt_max',\n",
    " 'zhongxun_page_id_target_max',\n",
    " 'group_age_page_id_target_sum_max_x',\n",
    " 'page_CCFSC_cnt',\n",
    " 'group_age_amt_all_min',\n",
    " 'day_5_amt_sum',\n",
    " 'day_30_amt_max',\n",
    " 'groupby_uid_date_cd_cnt_std',\n",
    " 'day_3_amt_sum',\n",
    " 'page_00FQX_cnt',\n",
    " 'page_F7FQM_cnt',\n",
    " 'day_24_amt_min',\n",
    " 'page_9CEPA_cnt',\n",
    " 'xiaxun_page_id_target_std',\n",
    " 'day_14_amt_min',\n",
    " 'shangxun_page_id_target_min',\n",
    " 'page_count',\n",
    " 'day_10_amt_mean',\n",
    " 'page_5D8QD_cnt',\n",
    " 'day_7_amt_mean',\n",
    " 'page_3D2DI_cnt',\n",
    " 'group_age_page_id_target_max_std_y',\n",
    " 'group_age_page_id_target_std_std_x',\n",
    " 'important_amt_std',\n",
    " 'acs_days_count',\n",
    " 'groupby_uid_pid_days_cnt_max',\n",
    " 'day_16_amt_min',\n",
    " 'day_6_amt_sum',\n",
    " 'day_31_amt_sum',\n",
    " 'zhongxun_page_id_target_sum',\n",
    " 'page_4CEZH_cnt',\n",
    " 'day_29_amt_min',\n",
    " 'page_F2FCE_cnt',\n",
    " 'day_20_amt_max',\n",
    " 'day_5_amt_min',\n",
    " 'shangxun_page_id_target_mean',\n",
    " 'shangxun_page_id_target_std',\n",
    " 'day_24_amt_max',\n",
    " 'day_3_amt_max',\n",
    " 'page_BFDTG_cnt',\n",
    " 'day_18_amt_mean',\n",
    " 'day_1_amt_std',\n",
    " 'page_1B7SG_cnt',\n",
    " 'day_13_amt_sum',\n",
    " 'group_age_page_id_target_max_mean_y',\n",
    " 'group_age_amt_mean_mean',\n",
    " 'day_6_amt_std',\n",
    " 'day_13_amt_min',\n",
    " 'page_FD7YY_cnt',\n",
    " 'page_D0DBB_cnt',\n",
    " 'page_819UJ_cnt',\n",
    " 'page_C8ABH_cnt',\n",
    " 'page_2C2GC_cnt',\n",
    " 'page_281TR_cnt',\n",
    " 'day_10_amt_std',\n",
    " 'day_21_amt_mean',\n",
    " 'day_2_amt_std',\n",
    " 'group_cd_cnt_max',\n",
    " 'day_24_amt_mean',\n",
    " 'page_77AZ5_cnt',\n",
    " 'page_FF2TR_cnt',\n",
    " 'day_12_amt_min',\n",
    " 'xiaxun_page_id_target_mean',\n",
    " 'group_age_page_id_target_min_min_y',\n",
    " 'day_11_amt_max',\n",
    " 'page_A67TR_cnt',\n",
    " 'group_age_page_id_target_min_min_x',\n",
    " 'group_age_amt_mean_std',\n",
    " 'day_24_amt_sum',\n",
    " 'day_1_amt_mean',\n",
    " 'page_06AWA_cnt',\n",
    " 'page_EB1IL_cnt',\n",
    " 'group_age_page_id_target_min_mean_x',\n",
    " 'amt_max',\n",
    " 'group_age_page_id_target_min_std_x',\n",
    " 'day_8_amt_mean',\n",
    " 'page_9D6AG_cnt',\n",
    " 'page_3AFC5_cnt',\n",
    " 'page_CB3DA_cnt',\n",
    " 'page_EF3OD_cnt',\n",
    " 'day_25_amt_sum',\n",
    " 'groupby_uid_cd_cnt_min',\n",
    " 'page_8A3Q1_cnt',\n",
    " 'groupby_uid_cd_cnt_std',\n",
    " 'page_2C4EM_cnt',\n",
    " 'day_31_amt_std',\n",
    " 'group_age_amt_all_mean',\n",
    " 'xiaxun_page_id_target_max',\n",
    " 'page_0C6WF_cnt',\n",
    " 'page_9D7GM_cnt',\n",
    " 'page_4C3JJ_cnt',\n",
    " 'group_cd_cnt_std',\n",
    " 'page_845Z1_cnt',\n",
    " 'page_64EAD_cnt',\n",
    " 'page_964WE_cnt',\n",
    " 'group_age_page_id_target_max_std_x',\n",
    " 'group_age_amt_min_min',\n",
    " 'groupby_uid_pid_days_cnt_mean',\n",
    " 'day_4_amt_max',\n",
    " 'group_age_amt_all_std',\n",
    " 'page_B4CSJ_cnt',\n",
    " 'groupby_uid_date_page_cnt_std',\n",
    " 'page_E76BS_cnt',\n",
    " 'day_16_amt_mean',\n",
    " 'day_28_amt_max',\n",
    " 'group_age_page_id_target_max_max_y',\n",
    " 'gdr_cd',\n",
    " 'day_9_amt_std',\n",
    " 'day_28_amt_min',\n",
    " 'day_17_amt_std',\n",
    " 'day_18_amt_max',\n",
    " 'day_12_amt_max',\n",
    " 'day_24_amt_std',\n",
    " 'day_25_amt_std',\n",
    " 'day_14_amt_std',\n",
    " 'day_16_amt_std',\n",
    " 'page_24EZC_cnt',\n",
    " 'day_10_amt_min',\n",
    " 'day_28_amt_mean',\n",
    " 'group_cd_uid_cnt_std',\n",
    " 'day_29_amt_sum',\n",
    " 'day_25_amt_max',\n",
    " 'page_2F2XA_cnt',\n",
    " 'day_17_amt_min',\n",
    " 'amt_all',\n",
    " 'group_age_amt_min_max',\n",
    " 'day_7_amt_max',\n",
    " 'day_9_amt_min',\n",
    " 'page_D22QC_cnt',\n",
    " 'zhongxun_page',\n",
    " 'page_E9FZX_cnt',\n",
    " 'groupby_uid_date_cd_cnt_mean',\n",
    " 'group_page_days_cnt_std',\n",
    " 'xiaxun_view_count',\n",
    " 'day_29_amt_std',\n",
    " 'page_C6ESC_cnt',\n",
    " 'max_oneday_pages',\n",
    " 'day_13_amt_max',\n",
    " 'day_27_amt_mean',\n",
    " 'group_age_amt_mean_max',\n",
    " 'page_B42DU_cnt',\n",
    " 'amt_mingroup_ages',\n",
    " 'day_18_amt_sum',\n",
    " 'cty_cd_num',\n",
    " 'page_2CAFH_cnt',\n",
    " 'day_19_amt_max',\n",
    " 'day_26_amt_min',\n",
    " 'page_503CW_cnt',\n",
    " 'group_age_page_id_target_min_std_y',\n",
    " 'page_9CBDJ_cnt',\n",
    " 'day_20_amt_min',\n",
    " 'groupby_uid_pid_cnt_std',\n",
    " 'day_18_amt_std',\n",
    " 'page_AD5XY_cnt',\n",
    " 'page_4A3KA_cnt',\n",
    " 'group_age_amt_min_std',\n",
    " 'important_page_cnt',\n",
    " 'page_B89CE_cnt',\n",
    " 'page_38CJL_cnt',\n",
    " 'day_29_amt_max',\n",
    " 'day_26_amt_max',\n",
    " 'day_30_amt_sum',\n",
    " 'groupby_uid_cd_days_cnt_std',\n",
    " 'shangxun_page_id_target_max',\n",
    " 'group_age_amt_max_mean',\n",
    " 'shangxun_trx_count',\n",
    " 'cty_cd_numgroup_ages',\n",
    " 'page_0DBZA_cnt',\n",
    " 'page_5FANH_cnt',\n",
    " 'max_oneday_trx',\n",
    " 'view_count',\n",
    " 'page_13EHB_cnt',\n",
    " 'day_6_amt_mean',\n",
    " 'page_8BFYD_cnt',\n",
    " 'group_age_page_id_target_sum_mean_x',\n",
    " 'group_cd_uid_cnt_max',\n",
    " 'page_3DBKD_cnt',\n",
    " 'page_B7BSA_cnt',\n",
    " 'page_989K1_cnt',\n",
    " 'age_bins',\n",
    " 'page_224HQ_cnt',\n",
    " 'day_25_amt_min',\n",
    " 'group_page_uid_cnt_std',\n",
    " 'day_27_amt_std',\n",
    " 'group_age_amt_all_max',\n",
    " 'groupby_uid_cd_days_cnt_mean',\n",
    " 'page_4B4GF_cnt',\n",
    " 'day_8_amt_max',\n",
    " 'shangxun_trx_amtmean',\n",
    " 'day_19_amt_std',\n",
    " 'page_E3COA_cnt',\n",
    " 'page_587QE_cnt',\n",
    " 'page_424CE_cnt',\n",
    " 'day_2_amt_mean',\n",
    " 'day_23_amt_std',\n",
    " 'day_9_amt_sum',\n",
    " 'page_85DJR_cnt',\n",
    " 'day_26_amt_sum',\n",
    " 'day_22_amt_std',\n",
    " 'day_12_amt_mean',\n",
    " 'page_E2BCE_cnt',\n",
    " 'page_8C2PT_cnt',\n",
    " 'day_27_amt_max',\n",
    " 'group_age_page_id_target_mean_max_y',\n",
    " 'page_549Z3_cnt',\n",
    " 'group_cd_uid_cnt_min',\n",
    " 'page_B93GE_cnt',\n",
    " 'important_amt_sum',\n",
    " 'day_23_amt_max',\n",
    " 'groupby_uid_date_page_cnt_max',\n",
    " 'page_73DZA_cnt',\n",
    " 'day_8_amt_std',\n",
    " 'page_71DJY_cnt',\n",
    " 'day_28_amt_std',\n",
    " 'day_11_amt_mean',\n",
    " 'group_age_page_id_target_mean_max_x',\n",
    " 'page_616KG_cnt',\n",
    " 'min_oneday_views',\n",
    " 'page_DC1IT_cnt',\n",
    " 'group_age_amt_std_std',\n",
    " 'day_13_amt_std',\n",
    " 'page_A38GB_cnt',\n",
    " 'page_52ED9_cnt',\n",
    " 'day_1_amt_max',\n",
    " 'zhongxun_view_count',\n",
    " 'zhongxun_page_id_target_std',\n",
    " 'day_15_amt_max',\n",
    " 'page_C4DJL_cnt',\n",
    " 'group_page_days_cnt_min',\n",
    " 'group_age_page_id_target_std_min_y',\n",
    " 'page_CE7XD_cnt',\n",
    " 'shangxun_trx_amtsum']\n",
    "#cate_feature = ['gdr_cd','cty_cd_cat','age_bins']\n",
    "test_X =test_T[use_cols]\n",
    "test_pred_prob = np.zeros((test_X.shape[0], ))\n",
    "for i in range(5):\n",
    "    clf = pickle.load(open(f'test_data/lgb_clf_{i}.pkl', 'rb'))\n",
    "    test_pred_prob+= clf.predict(test_X, num_iteration=clf.best_iteration) /5\n",
    "    plt.hist(clf.predict(test_X, num_iteration=clf.best_iteration))\n",
    "    plt.show()\n",
    "test_T['label'] = pd.Series(test_pred_prob).apply(lambda x:1 if x>0.081 else 0).values\n",
    "test_reg_X = test_T[test_T['label']==1][use_cols]\n",
    "#reg_pre = np.zeros((test_reg_X.shape[0], ))\n",
    "# for i in range(5):\n",
    "#     reg_i = pickle.load(open(f'test_data/lgb_reg_{i}.pkl', 'rb'))\n",
    "#     reg_pre+= reg_i.predict(test_reg_X, num_iteration=reg_i.best_iteration) /5\n",
    "#     #plt.hist(reg_i.predict(test_X, num_iteration=clf.best_iteration))\n",
    "#     #plt.show()\n",
    "\n",
    "reg_pre = lgb_reg.predict(test_reg_X)\n",
    "#这里是回归阶段的阈值选择,B榜十分坑爹,不能用线下的最优切分方法,其实B榜非0样本3是最多的,如果用线下的分法几乎没有预测为3的样本,会使分数很低\n",
    "#这里采用了一个试出来的切分方法,其实B榜非0样本全预测为3就能得到很好的线上MAPE,我做这个切分结果也就好了一点\n",
    "q1,q2,q3=(1.8031376247698658, 1.8951722591194298, 2.0291968200168173)\n",
    "\n",
    "test_T.loc[test_T['label'] != 0, 'label'] = np.array([1 if num < q1 else 2 if q1 <= num <= q2 else 3 if q2 < num <= q3 else 4 for num in reg_pre])\n",
    "\n",
    "test_T['label'] =test_T['label'].astype(int)\n",
    "res=test_T[['cust_wid','label']]\n",
    "res.sort_values('cust_wid', inplace = True)\n",
    "res = res.reset_index(drop = True)\n",
    "res.to_csv('/work/output.csv', encoding='latin_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
